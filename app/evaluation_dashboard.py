"""
LLMæ©Ÿèƒ½è©•ä¾¡å°‚ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
ä½¿ç”¨æ–¹æ³•: python -m streamlit run app/evaluation_dashboard.py
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import hashlib

from services.evaluation_service import EvaluationService, EvaluationResult
from models.report import DocumentReport
from services.document_processor import DocumentProcessor
from config.settings import DATA_DIR
import logging

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="LLMæ©Ÿèƒ½è©•ä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ğŸ¨ ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨åŒã˜ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
EVALUATION_STYLE = """
<style>
    /* ã‚·ã‚¹ãƒ†ãƒ åŸºèª¿ã‚«ãƒ©ãƒ¼ */
    :root {
        --primary-blue: #0052CC;
        --light-blue: #4A90E2;
        --dark-blue: #003C8F;
        --accent-orange: #FF6B35;
        --light-gray: #F5F7FA;
        --dark-gray: #2C3E50;
        --text-primary: #2C3E50;
        --text-secondary: #7F8C8D;
    }
    
    /* ãƒ¡ã‚¤ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ */
    .main-header {
        background: linear-gradient(135deg, var(--primary-blue) 0%, var(--light-blue) 100%);
        padding: 24px 32px;
        border-radius: 12px;
        color: white;
        margin-bottom: 24px;
        box-shadow: 0 4px 20px rgba(0, 82, 204, 0.25);
    }
    
    .main-header h1 {
        font-size: 36px;
        font-weight: 700;
        margin: 0 0 8px 0;
        line-height: 1.2;
    }
    
    .main-header p {
        font-size: 16px;
        opacity: 0.9;
        margin: 0;
        line-height: 1.4;
    }
    
    /* ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰ */
    .metric-card {
        background: white;
        padding: 24px;
        border-radius: 12px;
        border-left: 5px solid var(--primary-blue);
        box-shadow: 0 2px 12px rgba(0, 0, 0, 0.08);
        margin: 12px 0;
        transition: all 0.2s ease;
    }
    
    .metric-card:hover {
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.12);
        transform: translateY(-2px);
    }
    
    /* ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ */
    .custom-header {
        font-size: 26px;
        font-weight: 700;
        color: var(--primary-blue);
        margin: 24px 0 16px 0;
        padding-bottom: 8px;
        border-bottom: 2px solid var(--primary-blue);
        line-height: 1.3;
    }
    
    /* ãƒœã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton > button {
        background: var(--primary-blue);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 12px 24px;
        font-weight: 600;
        font-size: 14px;
        transition: all 0.3s ease;
        border: 2px solid var(--primary-blue);
    }
    
    .stButton > button:hover {
        background: var(--light-blue);
        border-color: var(--light-blue);
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(74, 144, 226, 0.4);
    }
    
    /* ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚¿ã‚¤ãƒ« */
    .dataframe {
        font-size: 14px;
    }
    
    /* è­¦å‘Šãƒ»æƒ…å ±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ */
    .stAlert {
        border-radius: 8px;
    }
</style>
"""

st.markdown(EVALUATION_STYLE, unsafe_allow_html=True)

def load_processed_reports() -> List:
    """äº‹å‰å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    try:
        processed_file = Path(DATA_DIR) / "processed_reports.json"
        if processed_file.exists():
            with open(processed_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('reports', [])
    except Exception as e:
        st.error(f"äº‹å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    return []

def _generate_evaluation_hash() -> str:
    """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”¨ï¼‰"""
    # äº‹å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
    processed_reports_dir = Path("data/processed_reports")
    index_file = processed_reports_dir / "index.json"
    
    data_hash = ""
    if index_file.exists():
        with open(index_file, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–
        file_info = []
        for file_path, info in index_data.get("processed_files", {}).items():
            if info.get("status") == "success":
                file_info.append({
                    "file_path": file_path,
                    "processed_at": info.get("processed_at"),
                    "file_hash": info.get("file_hash")
                })
        
        data_hash = hashlib.md5(str(sorted(file_info, key=lambda x: x["file_path"])).encode()).hexdigest()
    
    # æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
    ground_truth_file = Path("data/evaluation/ground_truth.json")
    gt_hash = ""
    if ground_truth_file.exists():
        with open(ground_truth_file, 'r', encoding='utf-8') as f:
            gt_data = json.load(f)
        gt_hash = hashlib.md5(str(gt_data).encode()).hexdigest()
    
    return hashlib.md5(f"{data_hash}_{gt_hash}".encode()).hexdigest()

def _deserialize_report_for_evaluation(data: Dict[str, Any]) -> Optional[DocumentReport]:
    """è©•ä¾¡ç”¨DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¾©å…ƒ"""
    try:
        from app.models.report import StatusFlag, CategoryLabel, RiskLevel, ConstructionStatus, AnalysisResult, AnomalyDetection, ReportType
        
        report = DocumentReport(
            file_path=data["file_path"],
            file_name=data["file_name"],
            report_type=ReportType(data["report_type"]) if data.get("report_type") else ReportType.PROGRESS_UPDATE,
            content=data.get("content", data.get("content_preview", "")),
            created_at=datetime.fromisoformat(data.get("processed_at", datetime.now().isoformat()))
        )
        
        # AnalysisResultå¾©å…ƒ
        if data.get("analysis_result"):
            analysis = data["analysis_result"]
            report.analysis_result = AnalysisResult(
                project_info={"project": "evaluation"},  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                status="unknown",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                issues=[],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                risk_level="ä½",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                summary="",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                urgency_score=1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                key_points=analysis.get("key_points", "").split(",") if analysis.get("key_points") else [],
                recommended_flags=analysis.get("recommended_flags", "").split(",") if analysis.get("recommended_flags") else [],
                confidence=float(analysis.get("confidence", 0.0))
            )
        
        # AnomalyDetectionå¾©å…ƒ
        if data.get("anomaly_detection"):
            anomaly = data["anomaly_detection"]
            report.anomaly_detection = AnomalyDetection(
                is_anomaly=bool(anomaly.get("has_anomaly", False)),
                anomaly_description=anomaly.get("explanation", ""),
                confidence=float(anomaly.get("anomaly_score", 0.0)),
                suggested_action="",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                requires_human_review=False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                similar_cases=[]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            )
        
        # æ–°ã—ã„ãƒ•ãƒ©ã‚°ä½“ç³»å¾©å…ƒ
        if data.get("status_flag"):
            report.status_flag = StatusFlag(data["status_flag"])
        if data.get("category_labels"):
            report.category_labels = [CategoryLabel(label) for label in data["category_labels"]]
        if data.get("risk_level"):
            report.risk_level = RiskLevel(data["risk_level"])
        if data.get("construction_status"):
            report.construction_status = ConstructionStatus(data["construction_status"])
        
        return report
        
    except Exception as e:
        logger.error(f"Failed to deserialize report for evaluation: {e}")
        return None

@st.cache_data(ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def _cached_run_evaluation(evaluation_hash: str) -> EvaluationResult:
    """è©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ + ä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
    processed_reports_dir = Path("data/processed_reports")
    
    if not processed_reports_dir.exists():
        st.error("âš ï¸ äº‹å‰å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        st.code("python scripts/preprocess_documents.py")
        return None
    
    # ğŸš€ ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ãŸä¸¦åˆ—èª­ã¿è¾¼ã¿
    try:
        from app.utils.cache_loader import CacheLoader
        import time
        
        cache_loader = CacheLoader(max_workers=3)  # è©•ä¾¡ç”¨ã¯å°‘ã—æ§ãˆã‚
        
        start_time = time.time()
        reports = cache_loader.load_reports_parallel(processed_reports_dir)
        load_time = time.time() - start_time
        
        if not reports:
            st.error("å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        st.info(f"{len(reports)}ä»¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’{load_time:.2f}ç§’ã§èª­ã¿è¾¼ã¿å®Œäº†")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡ç”¨ï¼‰
        if 'current_reports' not in st.session_state:
            st.session_state.current_reports = reports
        
        # è©•ä¾¡å®Ÿè¡Œ
        evaluator = EvaluationService()
        return evaluator.evaluate_reports(reports)
        
    except Exception as e:
        st.error(f"è©•ä¾¡å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        logger.error(f"Evaluation failed: {e}")
        return None

def run_evaluation() -> EvaluationResult:
    """è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    evaluation_hash = _generate_evaluation_hash()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
    cache_info = f"ğŸ”‘ Cache Key: {evaluation_hash[:8]}..."
    if evaluation_hash in st.session_state.get('evaluation_cache_keys', set()):
        cache_info += " âš¡ (Cached)"
    else:
        cache_info += " ğŸ”„ (Computing)"
    
    st.caption(cache_info)
    
    with st.spinner("è©•ä¾¡å®Ÿè¡Œä¸­...ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
        result = _cached_run_evaluation(evaluation_hash)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’è¨˜éŒ²
        if 'evaluation_cache_keys' not in st.session_state:
            st.session_state.evaluation_cache_keys = set()
        st.session_state.evaluation_cache_keys.add(evaluation_hash)
        
        return result

def render_metrics_overview(evaluation_result: EvaluationResult):
    """çµ±åˆåˆ†æè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>çµ±åˆåˆ†æè©•ä¾¡çµæœæ¦‚è¦</div>", unsafe_allow_html=True)
    
    # ç·åˆã‚¹ã‚³ã‚¢
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        st.metric(
            "ç·åˆã‚¹ã‚³ã‚¢",
            f"{evaluation_result.overall_score:.3f}",
            delta=None
        )
    
    with col2:
        st.metric(
            "ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ— F1",
            f"{evaluation_result.report_type_classification.f1_score:.3f}",
            delta=None
        )
    
    with col3:
        st.metric(
            "çŠ¶æ…‹åˆ†é¡ F1",
            f"{evaluation_result.status_classification.f1_score:.3f}",
            delta=None
        )
    
    with col4:
        st.metric(
            "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ F1",
            f"{evaluation_result.category_classification.f1_score:.3f}",
            delta=None
        )
    
    with col5:
        st.metric(
            "ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ« F1",
            f"{evaluation_result.risk_level_assessment.f1_score:.3f}",
            delta=None
        )
    
    with col6:
        st.metric(
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚° F1",
            f"{evaluation_result.project_mapping.f1_score:.3f}",
            delta=None
        )

def render_unified_analysis_results(evaluation_result: EvaluationResult):
    """çµ±åˆåˆ†æçµæœã®è©³ç´°è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>çµ±åˆåˆ†ææ©Ÿèƒ½è©•ä¾¡</div>", unsafe_allow_html=True)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ†é¡
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ†é¡")
        rt_metrics = evaluation_result.report_type_classification
        metrics_df = pd.DataFrame({
            'æŒ‡æ¨™': ['æ­£ç¢ºåº¦', 'é©åˆç‡', 'å†ç¾ç‡', 'F1ã‚¹ã‚³ã‚¢'],
            'å€¤': [rt_metrics.accuracy, rt_metrics.precision, rt_metrics.recall, rt_metrics.f1_score]
        })
        st.dataframe(metrics_df, use_container_width=True)
    
    with col2:
        st.subheader("äººæ‰‹ç¢ºèªæ¤œçŸ¥")
        hr_metrics = evaluation_result.human_review_detection
        metrics_df = pd.DataFrame({
            'æŒ‡æ¨™': ['æ­£ç¢ºåº¦', 'é©åˆç‡', 'å†ç¾ç‡', 'F1ã‚¹ã‚³ã‚¢'],
            'å€¤': [hr_metrics.accuracy, hr_metrics.precision, hr_metrics.recall, hr_metrics.f1_score]
        })
        st.dataframe(metrics_df, use_container_width=True)
    
    st.subheader("ğŸ”„ å‡¦ç†åŠ¹ç‡ã®æ”¹å–„åŠ¹æœ")
    
    # çµ±åˆåˆ†æã«ã‚ˆã‚‹æ”¹å–„åŠ¹æœã‚’è¡¨ç¤º
    improvement_data = {
        "é …ç›®": ["LLMå‘¼ã³å‡ºã—å›æ•°", "å‡¦ç†æ™‚é–“/ãƒ•ã‚¡ã‚¤ãƒ«", "ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦", "ãƒ‡ãƒ¼ã‚¿å“è³ª"],
        "å¾“æ¥": ["3å›", "2-3ç§’", "é«˜", "ä¸å®‰å®š"],
        "çµ±åˆåˆ†æ": ["1å›", "1-1.5ç§’", "ä½", "å®‰å®š"],
        "æ”¹å–„ç‡": ["67%å‰Šæ¸›", "50%å‰Šæ¸›", "å¤§å¹…æ”¹å–„", "å“è³ªå‘ä¸Š"]
    }
    
    improvement_df = pd.DataFrame(improvement_data)
    st.dataframe(improvement_df, use_container_width=True)

def render_detailed_metrics(evaluation_result: EvaluationResult):
    """è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>è©³ç´°è©•ä¾¡æŒ‡æ¨™</div>", unsafe_allow_html=True)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    metrics_data = {
        "æ©Ÿèƒ½": ["çŠ¶æ…‹åˆ†é¡", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è©•ä¾¡", "ç•°å¸¸æ¤œçŸ¥"],
        "ç²¾åº¦ (Accuracy)": [
            evaluation_result.status_classification.accuracy,
            evaluation_result.category_classification.accuracy,
            evaluation_result.risk_level_assessment.accuracy,
            evaluation_result.anomaly_detection.accuracy
        ],
        "é©åˆç‡ (Precision)": [
            evaluation_result.status_classification.precision,
            evaluation_result.category_classification.precision,
            evaluation_result.risk_level_assessment.precision,
            evaluation_result.anomaly_detection.precision
        ],
        "å†ç¾ç‡ (Recall)": [
            evaluation_result.status_classification.recall,
            evaluation_result.category_classification.recall,
            evaluation_result.risk_level_assessment.recall,
            evaluation_result.anomaly_detection.recall
        ],
        "F1ã‚¹ã‚³ã‚¢": [
            evaluation_result.status_classification.f1_score,
            evaluation_result.category_classification.f1_score,
            evaluation_result.risk_level_assessment.f1_score,
            evaluation_result.anomaly_detection.f1_score
        ]
    }
    
    df = pd.DataFrame(metrics_data)
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¡¨
    st.dataframe(
        df.style.format({
            "ç²¾åº¦ (Accuracy)": "{:.3f}",
            "é©åˆç‡ (Precision)": "{:.3f}",
            "å†ç¾ç‡ (Recall)": "{:.3f}",
            "F1ã‚¹ã‚³ã‚¢": "{:.3f}"
        }).background_gradient(cmap='RdYlGn', vmin=0, vmax=1),
        use_container_width=True
    )

def render_performance_charts(evaluation_result: EvaluationResult):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦–è¦šåŒ–</div>", unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
        categories = ['çŠ¶æ…‹åˆ†é¡', 'ã‚«ãƒ†ã‚´ãƒªåˆ†é¡', 'ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«', 'ç•°å¸¸æ¤œçŸ¥']
        f1_scores = [
            evaluation_result.status_classification.f1_score,
            evaluation_result.category_classification.f1_score,
            evaluation_result.risk_level_assessment.f1_score,
            evaluation_result.anomaly_detection.f1_score
        ]
        
        fig_radar = go.Figure()
        fig_radar.add_trace(go.Scatterpolar(
            r=f1_scores,
            theta=categories,
            fill='toself',
            name='F1ã‚¹ã‚³ã‚¢',
            line_color='rgb(1,90,180)'
        ))
        
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=False,
            title="æ©Ÿèƒ½åˆ¥F1ã‚¹ã‚³ã‚¢",
            height=400
        )
        
        st.plotly_chart(fig_radar, use_container_width=True)
    
    with col2:
        # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆç²¾åº¦ã€å†ç¾ç‡ã€F1ï¼‰
        metrics = ['ç²¾åº¦', 'å†ç¾ç‡', 'F1ã‚¹ã‚³ã‚¢']
        status_metrics = [
            evaluation_result.status_classification.accuracy,
            evaluation_result.status_classification.recall,
            evaluation_result.status_classification.f1_score
        ]
        category_metrics = [
            evaluation_result.category_classification.accuracy,
            evaluation_result.category_classification.recall,
            evaluation_result.category_classification.f1_score
        ]
        
        fig_bar = go.Figure()
        fig_bar.add_trace(go.Bar(
            name='çŠ¶æ…‹åˆ†é¡',
            x=metrics,
            y=status_metrics,
            marker_color='lightblue'
        ))
        fig_bar.add_trace(go.Bar(
            name='ã‚«ãƒ†ã‚´ãƒªåˆ†é¡',
            x=metrics,
            y=category_metrics,
            marker_color='lightgreen'
        ))
        
        fig_bar.update_layout(
            title="ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ",
            yaxis_title="ã‚¹ã‚³ã‚¢",
            barmode='group',
            height=400
        )
        
        st.plotly_chart(fig_bar, use_container_width=True)

def render_confusion_matrix(evaluation_result: EvaluationResult):
    """æ··åŒè¡Œåˆ—è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>æ··åŒè¡Œåˆ—</div>", unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**çŠ¶æ…‹åˆ†é¡ã®æ··åŒè¡Œåˆ—**")
        if evaluation_result.status_classification.confusion_matrix:
            cm_data = evaluation_result.status_classification.confusion_matrix
            st.json(cm_data)
    
    with col2:
        st.write("**ç•°å¸¸æ¤œçŸ¥ã®æ··åŒè¡Œåˆ—**")
        if evaluation_result.anomaly_detection.confusion_matrix:
            cm_data = evaluation_result.anomaly_detection.confusion_matrix
            
            # ãƒã‚¤ãƒŠãƒªåˆ†é¡ã®æ··åŒè¡Œåˆ—ã‚’è¦–è¦šåŒ–
            tp = cm_data.get('true_positive', 0)
            fp = cm_data.get('false_positive', 0)
            fn = cm_data.get('false_negative', 0)
            tn = cm_data.get('true_negative', 0)
            
            cm_matrix = [[tn, fp], [fn, tp]]
            
            fig_cm = px.imshow(
                cm_matrix,
                labels=dict(x="äºˆæ¸¬", y="å®Ÿéš›", color="ä»¶æ•°"),
                x=['æ­£å¸¸', 'ç•°å¸¸'],
                y=['æ­£å¸¸', 'ç•°å¸¸'],
                text_auto=True,
                color_continuous_scale='Blues'
            )
            fig_cm.update_layout(title="ç•°å¸¸æ¤œçŸ¥æ··åŒè¡Œåˆ—", height=300)
            st.plotly_chart(fig_cm, use_container_width=True)

def render_project_mapping_evaluation():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡</div>", unsafe_allow_html=True)
    
    if 'current_reports' not in st.session_state:
        st.warning("ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    try:
        from services.evaluation_service import EvaluationService
        evaluation_service = EvaluationService()
        reports = st.session_state.current_reports
        
        mapping_metrics = evaluation_service.evaluate_project_mapping(reports)
        
        if mapping_metrics.accuracy == 0 and not mapping_metrics.confusion_matrix:
            st.info("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡å¯¾è±¡ã®ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "ãƒãƒƒãƒ”ãƒ³ã‚°ç²¾åº¦", 
                f"{mapping_metrics.accuracy:.1%}",
                help="æ­£ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸå‰²åˆ"
            )
        with col2:
            st.metric(
                "å¹³å‡ä¿¡é ¼åº¦", 
                f"{mapping_metrics.precision:.1%}",
                help="ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¹³å‡ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢"
            )
        with col3:
            method_count = len(mapping_metrics.confusion_matrix)
            st.metric(
                "è©•ä¾¡å¯¾è±¡æ‰‹æ³•æ•°", 
                method_count,
                help="ä½¿ç”¨ã•ã‚ŒãŸãƒãƒƒãƒ”ãƒ³ã‚°æ‰‹æ³•ã®ç¨®é¡æ•°"
            )
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°æˆ¦ç•¥ã®èª¬æ˜
        st.divider()
        st.markdown("**ğŸ¯ ãƒãƒƒãƒ”ãƒ³ã‚°æˆ¦ç•¥**")
        
        strategy_info = {
            "direct_id": ["ç›´æ¥IDæŠ½å‡º", "æ–‡æ›¸ã‹ã‚‰å·¥äº‹ç•ªå·ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’ç›´æ¥æŠ½å‡º", "é«˜ï¼ˆ95%+ï¼‰", "æ˜ç¢ºãªIDè¨˜è¼‰ã‚ã‚Š"],
            "fuzzy_matching": ["ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãƒ»å ´æ‰€ãƒ»æœŸé–“ã®é¡ä¼¼åº¦ã§ãƒãƒƒãƒ”ãƒ³ã‚°", "ä¸­ï¼ˆ70-85%ï¼‰", "IDè¨˜è¼‰ãªã—ã€åç§°ãƒ»å ´æ‰€æƒ…å ±ã‚ã‚Š"]
        }
        
        strategy_df = pd.DataFrame.from_dict(strategy_info, orient='index', columns=["åç§°", "èª¬æ˜", "æœŸå¾…ç²¾åº¦", "é©ç”¨æ¡ä»¶"])
        st.dataframe(strategy_df, use_container_width=True, hide_index=True)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ
        st.divider()
        st.markdown("**ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ**")
        
        project_mapping_details = []
        for report in reports:
            file_name = report.file_name
            actual_project = getattr(report, 'project_id', None)
            mapping_info = getattr(report, 'project_mapping_info', {})
            
            confidence = mapping_info.get('confidence_score', 0.0)
            method = mapping_info.get('matching_method', 'unknown')
            
            # æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ
            expected_project = None
            if file_name in evaluation_service.ground_truth.get("evaluation_data", {}):
                expected_project = evaluation_service.ground_truth["evaluation_data"][file_name].get("expected_project_id")
            
            is_correct = actual_project == expected_project if expected_project else None
            
            project_mapping_details.append({
                "ãƒ•ã‚¡ã‚¤ãƒ«å": file_name[:30] + "..." if len(file_name) > 30 else file_name,
                "æŠ½å‡ºID": actual_project or "ãªã—",
                "æ­£è§£ID": expected_project or "æœªå®šç¾©",
                "æ‰‹æ³•": method,
                "ä¿¡é ¼åº¦": f"{confidence:.1%}",
                "çµæœ": "âœ… æ­£è§£" if is_correct else ("âŒ ä¸æ­£è§£" if is_correct is False else "âšª æœªè©•ä¾¡")
            })
        
        if project_mapping_details:
            mapping_df = pd.DataFrame(project_mapping_details)
            st.dataframe(mapping_df, use_container_width=True, hide_index=True)
            
            # å•é¡Œã®ã‚ã‚‹ã‚±ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            incorrect_cases = [case for case in project_mapping_details if "âŒ" in case["çµæœ"]]
            if incorrect_cases:
                st.warning(f"å•é¡Œã®ã‚ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°: {len(incorrect_cases)}ä»¶")
                with st.expander("è©³ç´°ã‚’ç¢ºèª"):
                    for case in incorrect_cases:
                        st.write(f"**{case['ãƒ•ã‚¡ã‚¤ãƒ«å']}**: {case['æŠ½å‡ºID']} â†’ {case['æ­£è§£ID']} (ä¿¡é ¼åº¦: {case['ä¿¡é ¼åº¦']})")
        
    except Exception as e:
        st.error(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"Project mapping evaluation error: {e}")

def render_sample_data_overview():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¦‚è¦"""
    st.markdown("<div class='custom-header'>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¦‚è¦</div>", unsafe_allow_html=True)
    
    try:
        ground_truth_file = Path(DATA_DIR) / "evaluation" / "ground_truth.json"
        if ground_truth_file.exists():
            with open(ground_truth_file, 'r', encoding='utf-8') as f:
                ground_truth = json.load(f)
            
            metadata = ground_truth.get('metadata', {})
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ**")
                if 'distribution' in metadata:
                    dist_data = metadata['distribution']
                    fig_status = px.pie(
                        values=list(dist_data.values()),
                        names=list(dist_data.keys()),
                        title="çŠ¶æ…‹ãƒ•ãƒ©ã‚°åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig_status, use_container_width=True)
            
            with col2:
                st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ**")
                if 'categories' in metadata:
                    cat_data = metadata['categories']
                    fig_cat = px.bar(
                        x=list(cat_data.keys()),
                        y=list(cat_data.values()),
                        title="åŸå› ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig_cat, use_container_width=True)
            
            st.write(f"**ç·ãƒ¬ãƒãƒ¼ãƒˆæ•°**: {metadata.get('total_reports', 0)}")
            st.write(f"**ä½œæˆæ—¥**: {metadata.get('created', 'N/A')}")
            
    except Exception as e:
        st.error(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    st.title("ğŸ”¬ LLMæ©Ÿèƒ½è©•ä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.caption("Aurora-LLM ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½è©•ä¾¡ã¨æ€§èƒ½åˆ†æ")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ›ï¸ è©•ä¾¡è¨­å®š")
        
        if st.button("ğŸš€ è©•ä¾¡å®Ÿè¡Œ", type="primary"):
            try:
                evaluation_result = run_evaluation()
                st.session_state.evaluation_result = evaluation_result
                st.success("è©•ä¾¡å®Œäº†ï¼")
            except Exception as e:
                st.error(f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        st.divider()
        
        st.info("""
        **è©•ä¾¡é …ç›®**
        - çŠ¶æ…‹åˆ†é¡ç²¾åº¦
        - ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç²¾åº¦  
        - ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è©•ä¾¡
        - ç•°å¸¸æ¤œçŸ¥æ€§èƒ½
        - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ç²¾åº¦
        """)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©•ä¾¡æƒ…å ±
        if 'current_reports' in st.session_state:
            reports = st.session_state.current_reports
            mapping_count = len([r for r in reports if hasattr(r, 'project_id') and r.project_id])
            st.markdown("**ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°**")
            st.write(f"ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿: {mapping_count}ä»¶")
            st.write(f"ç·ãƒ¬ãƒãƒ¼ãƒˆæ•°: {len(reports)}ä»¶")
            
            # ä¿¡é ¼åº¦åˆ†å¸ƒ
            confidence_scores = []
            for r in reports:
                if hasattr(r, 'project_mapping_info') and r.project_mapping_info:
                    confidence_scores.append(r.project_mapping_info.get('confidence_score', 0))
            
            if confidence_scores:
                avg_confidence = sum(confidence_scores) / len(confidence_scores)
                st.write(f"å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.1%}")
        else:
            st.markdown("**ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°**")
            st.write("è©•ä¾¡å®Ÿè¡Œå¾Œã«è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    if 'evaluation_result' in st.session_state:
        evaluation_result = st.session_state.evaluation_result
        
        # ã‚¿ãƒ–æ§‹æˆ
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["æ¦‚è¦", "çµ±åˆåˆ†æçµæœ", "è©³ç´°æŒ‡æ¨™", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"])
        
        with tab1:
            render_metrics_overview(evaluation_result)
        
        with tab2:
            render_unified_analysis_results(evaluation_result)
        
        with tab3:
            render_detailed_metrics(evaluation_result)
            st.divider()
            render_confusion_matrix(evaluation_result)
        
        with tab4:
            render_project_mapping_evaluation()
        
        with tab5:
            render_performance_charts(evaluation_result)
        
        with tab6:
            render_sample_data_overview()
    
    else:
        st.info("å·¦å´ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œè©•ä¾¡å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©•ä¾¡ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã ã‘è¡¨ç¤º
        render_sample_data_overview()

if __name__ == "__main__":
    main()