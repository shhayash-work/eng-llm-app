"""
ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
"""
import pickle
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from app.models.report import DocumentReport

logger = logging.getLogger(__name__)

class CacheLoader:
    """ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
    
    def load_reports_parallel(self, processed_reports_dir: Path) -> List[DocumentReport]:
        """
        ä¸¦åˆ—ã§ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        
        Args:
            processed_reports_dir: å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            List[DocumentReport]: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒªã‚¹ãƒˆ
        """
        start_time = time.time()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        index_file = processed_reports_dir / "index.json"
        if not index_file.exists():
            logger.warning(f"Index file not found: {index_file}")
            return []
        
        with open(index_file, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        successful_files = {k: v for k, v in index_data.get("processed_files", {}).items() 
                          if v.get("status") == "success"}
        
        logger.info(f"ğŸ” Found {len(successful_files)} processed files")
        
        # ä¸¦åˆ—èª­ã¿è¾¼ã¿ã®æº–å‚™
        cache_files = []
        fallback_files = []
        
        for file_path, file_info in successful_files.items():
            cache_file_path = file_info.get("cache_file")
            json_file_path = file_info.get("result_file")
            
            if cache_file_path:
                cache_file = Path(cache_file_path)
                json_file = Path(json_file_path) if json_file_path else None
                
                if cache_file.exists():
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–°ã—ã•ã‚’ãƒã‚§ãƒƒã‚¯
                    if json_file and json_file.exists():
                        cache_mtime = cache_file.stat().st_mtime
                        json_mtime = json_file.stat().st_mtime
                        
                        if cache_mtime >= json_mtime:
                            cache_files.append(cache_file)
                        else:
                            # JSONã®æ–¹ãŒæ–°ã—ã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            fallback_files.append((json_file, cache_file))
                            logger.debug(f"Cache outdated, using JSON: {json_file.name}")
                    else:
                        cache_files.append(cache_file)
                else:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯JSONã‹ã‚‰èª­ã¿è¾¼ã¿
                    if json_file and json_file.exists():
                        fallback_files.append((json_file, cache_file if cache_file_path else None))
                        logger.debug(f"Cache missing, using JSON: {json_file.name}")
        
        reports = []
        
        # ğŸš€ ä¸¦åˆ—ã§ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
        if cache_files:
            cache_reports = self._load_cache_files_parallel(cache_files)
            reports.extend(cache_reports)
            logger.info(f"âš¡ Loaded {len(cache_reports)} reports from binary cache")
        
        # ğŸ“„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆï¼‰
        if fallback_files:
            fallback_reports = self._load_json_files_with_cache_generation(fallback_files)
            reports.extend(fallback_reports)
            logger.info(f"ğŸ“„ Loaded {len(fallback_reports)} reports from JSON (with cache generation)")
        
        load_time = time.time() - start_time
        logger.info(f"ğŸ Total load time: {load_time:.3f}ç§’ ({len(reports)} reports)")
        
        return reports
    
    def _load_cache_files_parallel(self, cache_files: List[Path]) -> List[DocumentReport]:
        """ä¸¦åˆ—ã§ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        reports = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—ã§èª­ã¿è¾¼ã¿
            future_to_file = {
                executor.submit(self._load_single_cache_file, cache_file): cache_file
                for cache_file in cache_files
            }
            
            for future in as_completed(future_to_file):
                cache_file = future_to_file[future]
                try:
                    report = future.result()
                    if report:
                        reports.append(report)
                        logger.debug(f"âœ… Cache loaded: {cache_file.name}")
                except Exception as e:
                    logger.error(f"âŒ Failed to load cache {cache_file}: {e}")
        
        return reports
    
    def _load_single_cache_file(self, cache_file: Path) -> Optional[DocumentReport]:
        """å˜ä¸€ã®ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆJSONãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ Cache file corrupted: {cache_file.name}. Attempting JSON fallback...")
            
            # ç ´æã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            try:
                cache_file.unlink(missing_ok=True)
                logger.debug(f"ğŸ—‘ï¸ Removed corrupted cache: {cache_file.name}")
            except Exception:
                pass
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èª­ã¿è¾¼ã¿
            json_file = cache_file.with_suffix('.json')
            if json_file.exists():
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        report_data = json.load(f)
                    
                    report = self._deserialize_report(report_data)
                    if report:
                        # æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
                        try:
                            with open(cache_file, 'wb') as f:
                                pickle.dump(report, f)
                            logger.info(f"ğŸ”„ Regenerated cache from JSON: {cache_file.name}")
                        except Exception as cache_e:
                            logger.warning(f"Failed to regenerate cache {cache_file}: {cache_e}")
                        return report
                except Exception as json_e:
                    logger.error(f"âŒ JSON fallback also failed for {json_file}: {json_e}")
            
            logger.error(f"âŒ Complete failure to load {cache_file.name}")
            return None
    
    def _load_json_files_with_cache_generation(self, fallback_files: List[tuple]) -> List[DocumentReport]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€åŒæ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        reports = []
        
        for json_file, cache_file in fallback_files:
            try:
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                with open(json_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                
                # DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                report = self._deserialize_report(report_data)
                if report:
                    reports.append(report)
                    
                    # ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ¬¡å›ç”¨ï¼‰
                    if cache_file:
                        try:
                            cache_file.parent.mkdir(parents=True, exist_ok=True)
                            with open(cache_file, 'wb') as f:
                                pickle.dump(report, f)
                            logger.debug(f"ğŸ’¾ Generated cache: {cache_file.name}")
                        except Exception as e:
                            logger.warning(f"Failed to generate cache {cache_file}: {e}")
                
            except Exception as e:
                logger.error(f"Failed to load JSON file {json_file}: {e}")
        
        return reports
    
    def load_report_smart(self, json_path: Path) -> Optional[DocumentReport]:
        """JSONã¾ãŸã¯ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€å¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°"""
        cache_path = json_path.with_suffix('.cache')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ã€JSONã‚ˆã‚Šæ–°ã—ã„å ´åˆã€ã¾ãŸã¯JSONãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if cache_path.exists() and (not json_path.exists() or cache_path.stat().st_mtime >= json_path.stat().st_mtime):
            try:
                with open(cache_path, 'rb') as f:
                    report = pickle.load(f)
                logger.debug(f"âš¡ Loaded from binary cache: {json_path.name}")
                return report
            except Exception as e:
                logger.warning(f"Failed to load from cache {cache_path}: {e}. Falling back to JSON.")
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå£Šã‚Œã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤ã—ã¦JSONã‹ã‚‰å†ç”Ÿæˆ
                cache_path.unlink(missing_ok=True)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                report = self._deserialize_report(data)
                
                if report:
                    try:
                        with open(cache_path, 'wb') as f:
                            pickle.dump(report, f)
                        logger.debug(f"ğŸ’¾ Generated new binary cache for: {json_path.name}")
                    except Exception as e:
                        logger.warning(f"Failed to save binary cache {cache_path}: {e}")
                return report
            except Exception as e:
                logger.error(f"Failed to load from JSON {json_path}: {e}")
        
        return None
    
    def _deserialize_report(self, data: Dict[str, Any]) -> Optional[DocumentReport]:
        """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¾©å…ƒ"""
        try:
            from app.models.report import StatusFlag, RiskLevel, ConstructionStatus, AnalysisResult, AnomalyDetection, ReportType
            from datetime import datetime
            
            report = DocumentReport(
                file_path=data["file_path"],
                file_name=data["file_name"],
                report_type=ReportType(data["report_type"]) if data.get("report_type") else ReportType.PROGRESS_UPDATE,
                content=data.get("content", data.get("content_preview", "")),
                created_at=datetime.fromisoformat(data.get("processed_at", datetime.now().isoformat())),
                project_id=data.get("project_id")  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDå¾©å…ƒ
            )
            
            # AnalysisResultå¾©å…ƒ
            if data.get("analysis_result"):
                analysis = data["analysis_result"]
                report.analysis_result = AnalysisResult(
                    summary=analysis.get("summary", ""),
                    issues=analysis.get("issues", []),
                    key_points=analysis.get("key_points", "").split(",") if analysis.get("key_points") else [],
                    confidence=float(analysis.get("confidence", 0.0))
                )
            
            # AnomalyDetectionå¾©å…ƒï¼ˆæ–°æ§‹é€ ï¼‰
            if data.get("anomaly_detection"):
                anomaly = data["anomaly_detection"]
                report.anomaly_detection = AnomalyDetection(
                    is_anomaly=bool(anomaly.get("is_anomaly", anomaly.get("has_anomaly", False))),  # å¾Œæ–¹äº’æ›æ€§
                    anomaly_description=anomaly.get("anomaly_description", anomaly.get("explanation", "")),  # å¾Œæ–¹äº’æ›æ€§
                    confidence=float(anomaly.get("confidence", 0.0)),
                    suggested_action=anomaly.get("suggested_action", ""),
                    requires_human_review=bool(anomaly.get("requires_human_review", False)),
                    similar_cases=anomaly.get("similar_cases", [])
                )
            
            # æ–°ã—ã„ãƒ•ãƒ©ã‚°ä½“ç³»å¾©å…ƒ
            if data.get("status_flag"):
                report.status_flag = StatusFlag(data["status_flag"])
            # category_labelså‰Šé™¤: 15ã‚«ãƒ†ã‚´ãƒªé…å»¶ç†ç”±ä½“ç³»ã«çµ±ä¸€
            if data.get("risk_level"):
                report.risk_level = RiskLevel(data["risk_level"])
            if data.get("construction_status"):
                report.construction_status = ConstructionStatus(data["construction_status"])
            
            # ğŸš¨ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¾©å…ƒ
            report.has_unexpected_values = data.get("has_unexpected_values", False)
            report.validation_issues = data.get("validation_issues", [])
            
            # ğŸ¤– çµ±åˆåˆ†æçµæœãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¾©å…ƒ
            report.requires_human_review = data.get("requires_human_review", False)
            report.analysis_confidence = data.get("analysis_confidence", 0.0)
            # analysis_noteså‰Šé™¤: summaryã«çµ±åˆ
            
            # ğŸ” å»ºè¨­å·¥ç¨‹æƒ…å ±å¾©å…ƒ
            report.current_construction_phase = data.get("current_construction_phase")
            report.construction_progress = data.get("construction_progress")
            
            # ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°è©³ç´°æƒ…å ±å¾©å…ƒ
            report.project_mapping_info = data.get("project_mapping_info")
            
            # ğŸš§ é…å»¶ç†ç”±æƒ…å ±å¾©å…ƒï¼ˆ15ã‚«ãƒ†ã‚´ãƒªä½“ç³»ï¼‰
            report.delay_reasons = data.get("delay_reasons", [])
            
            # ğŸ¯ ç·Šæ€¥åº¦ã‚¹ã‚³ã‚¢å¾©å…ƒ
            report.urgency_score = data.get("urgency_score", 1)
            
            # current_statuså‰Šé™¤: status_flagã§çµ±ä¸€
            
            return report
            
        except Exception as e:
            logger.error(f"Failed to deserialize report: {e}")
            return None