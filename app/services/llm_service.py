"""
ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œLLMã‚µãƒ¼ãƒ“ã‚¹ - Ollama/OpenAI/Anthropic
"""
import json
import logging
import os
from typing import Dict, Any, Optional
from dotenv import load_dotenv
import streamlit as st

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

try:
    import ollama
except ImportError:
    ollama = None

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    ChatOpenAI = None

try:
    from langchain_anthropic import ChatAnthropic
except ImportError:
    ChatAnthropic = None

from app.config.settings import (
    LLM_PROVIDER,
    OLLAMA_MODEL, 
    OLLAMA_BASE_URL,
    OPENAI_API_KEY,
    OPENAI_MODEL,
    ANTHROPIC_API_KEY,
    ANTHROPIC_MODEL
)
from app.config.prompts import (
    SYSTEM_PROMPT, 
    DOCUMENT_ANALYSIS_PROMPT, 
    QA_PROMPT,
    FEW_SHOT_EXAMPLES
)

logger = logging.getLogger(__name__)

class LLMService:
    """ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œLLMã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, provider: Optional[str] = None, force_test: bool = False):
        self.provider = provider or LLM_PROVIDER
        self.model = None
        self.force_test = force_test
        self._setup_provider()
    
    def _setup_provider(self):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            if self.provider == "ollama":
                self._setup_ollama()
            elif self.provider == "openai":
                self._setup_openai()
            elif self.provider == "anthropic":
                self._setup_anthropic()
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"Failed to setup {self.provider}: {e}")
            self.model = None
    
    def _setup_ollama(self):
        """Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if ollama is None:
            raise ImportError("ollama package not installed")
        
        self.model = OLLAMA_MODEL
        self.client = ollama.Client(host=OLLAMA_BASE_URL)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸæ¥ç¶šãƒ†ã‚¹ãƒˆ
        self._test_ollama_connection()
    
    def _test_ollama_connection(self):
        """Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        # Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰
        try:
            import streamlit as st
            cache_key = f"ollama_tested_{OLLAMA_BASE_URL}_{OLLAMA_MODEL}"
            
            # å¼·åˆ¶ãƒ†ã‚¹ãƒˆã§ãªã„å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if not self.force_test and cache_key in st.session_state:
                cached_result = st.session_state[cache_key]
                self.model = cached_result.get("model", OLLAMA_MODEL)
                logger.info(f"âš¡ Using cached Ollama connection: {self.model}")
                return
        except ImportError:
            # StreamlitãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯æ¯å›ãƒ†ã‚¹ãƒˆ
            pass
        
        # å®Ÿéš›ã®æ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        try:
            logger.info(f"ğŸ” Testing Ollama connection: {OLLAMA_BASE_URL}")
            
            # Ollamaã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šç¢ºèª
            models = self.client.list()
            logger.info(f"Ollama server connected: {OLLAMA_BASE_URL}")
            logger.debug(f"Ollama models response: {models}")
            
            # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
            model_names = []
            try:
                if hasattr(models, 'models'):
                    models_list = models.models
                elif isinstance(models, dict) and 'models' in models:
                    models_list = models['models']
                elif isinstance(models, list):
                    models_list = models
                else:
                    models_list = []
                
                for model in models_list:
                    if hasattr(model, 'model'):
                        model_names.append(model.model)
                    elif hasattr(model, 'name'):
                        model_names.append(model.name)
                    elif isinstance(model, dict):
                        model_name = model.get('name') or model.get('model')
                        if model_name:
                            model_names.append(model_name)
            except Exception as e:
                logger.error(f"Failed to parse models list: {e}")
                model_names = []
                
            if self.model not in model_names:
                logger.warning(f"Model {self.model} not found. Available models: {model_names}")
                if model_names:
                    self.model = model_names[0]
                    logger.info(f"Using available model: {self.model}")
                else:
                    raise Exception("No models available in Ollama")
            else:
                logger.info(f"âœ… Using model: {self.model}")
            
            # æ¥ç¶šæˆåŠŸã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            try:
                import streamlit as st
                st.session_state[cache_key] = {
                    "model": self.model,
                    "status": "connected"
                }
                logger.info(f"ğŸ’¾ Cached Ollama connection result")
            except ImportError:
                pass
                
        except Exception as e:
            logger.error(f"Failed to connect to Ollama at {OLLAMA_BASE_URL}: {e}")
            logger.info("Please ensure Ollama is running with: ollama serve")
            raise
    
    def _setup_openai(self):
        """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if ChatOpenAI is None:
            raise ImportError("langchain-openai package not installed")
        
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not provided")
        
        self.model = OPENAI_MODEL
        self.client = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            model=OPENAI_MODEL,
            temperature=0.2
        )
        logger.info(f"OpenAI client initialized: {OPENAI_MODEL}")
    
    def _setup_anthropic(self):
        """Anthropicã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if ChatAnthropic is None:
            raise ImportError("langchain-anthropic package not installed")
        
        if not ANTHROPIC_API_KEY:
            raise ValueError("ANTHROPIC_API_KEY not provided")
        
        self.model = ANTHROPIC_MODEL
        self.client = ChatAnthropic(
            api_key=ANTHROPIC_API_KEY,
            model=ANTHROPIC_MODEL,
            temperature=0.2
        )
        logger.info(f"Anthropic client initialized: {ANTHROPIC_MODEL}")
    
    def _make_request(self, prompt: str, system_prompt: str = SYSTEM_PROMPT) -> str:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        if self.model is None:
            raise RuntimeError(f"LLM model not initialized for provider: {self.provider}")
        
        try:
            if self.provider == "ollama":
                return self._ollama_request(prompt, system_prompt)
            elif self.provider in ["openai", "anthropic"]:
                return self._langchain_request(prompt, system_prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"LLM request failed ({self.provider}): {e}")
            raise
    
    def _make_request_stream(self, prompt: str, system_prompt: str = SYSTEM_PROMPT):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        if self.model is None:
            raise RuntimeError(f"LLM model not initialized for provider: {self.provider}")
        
        try:
            if self.provider == "ollama":
                yield from self._ollama_request_stream(prompt, system_prompt)
            elif self.provider in ["openai", "anthropic"]:
                # OpenAI/Anthropicã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯å¾Œã§å®Ÿè£…
                yield self._langchain_request(prompt, system_prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"LLM streaming request failed ({self.provider}): {e}")
            yield f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _ollama_request(self, prompt: str, system_prompt: str) -> str:
        """Ollamaãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        response = self.client.chat(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            options={
                "temperature": 0.2,
                "top_p": 0.9,
                "num_predict": 3072,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦èª¿æ•´
                "num_ctx": 16384
            }
        )
        return response['message']['content']
    
    def _ollama_request_stream(self, prompt: str, system_prompt: str):
        """Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        stream = self.client.chat(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            options={
                "temperature": 0.2,
                "top_p": 0.9,
                "num_predict": 3072,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦èª¿æ•´
                "num_ctx": 16384
            },
            stream=True
        )
        
        for chunk in stream:
            if 'message' in chunk and 'content' in chunk['message']:
                yield chunk['message']['content']
    
    def _langchain_request(self, prompt: str, system_prompt: str) -> str:
        """LangChainãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆOpenAI/Anthropicï¼‰"""
        messages = [
            ("system", system_prompt),
            ("human", prompt)
        ]
        response = self.client.invoke(messages)
        return response.content
    
    def analyze_document(self, document_content: str) -> Dict[str, Any]:
        """æ–‡æ›¸ã‚’åˆ†æã—ã¦JSONçµæœã‚’è¿”ã™"""
        prompt = DOCUMENT_ANALYSIS_PROMPT.format(
            document_content=document_content
        )
        
        # Few-shotä¾‹æ–‡ã‚’å«ã‚ã‚‹
        full_prompt = f"{FEW_SHOT_EXAMPLES}\n\n{prompt}"
        
        try:
            response = self._make_request(full_prompt)
            logger.debug(f"LLM response: {response[:200]}...")
            
            # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦è¡Œ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                
                # JSONã®å‰å‡¦ç†ï¼ˆä¸€èˆ¬çš„ãªå•é¡Œã‚’ä¿®æ­£ï¼‰
                json_str = self._clean_json_string(json_str)
                
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as json_e:
                    logger.warning(f"JSON parse failed, attempting repair: {str(json_e)[:100]}")
                    # JSONä¿®å¾©ã‚’è©¦è¡Œ
                    repaired_json = self._repair_json_string(json_str)
                    if repaired_json:
                        try:
                            return json.loads(repaired_json)
                        except json.JSONDecodeError:
                            logger.warning("JSON repair failed, using fallback")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return self._fallback_analysis(document_content, response)
            else:
                # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆè§£æã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning("No JSON found in response, using fallback")
                return self._fallback_analysis(document_content, response)
                
        except Exception as e:
            logger.error(f"Document analysis failed: {e}")
            return self._create_error_result(str(e))

    
    def answer_question(self, question: str, context: str) -> str:
        """è³ªå•å¿œç­”æ©Ÿèƒ½"""
        prompt = QA_PROMPT.format(
            question=question,
            context=context
        )
        
        try:
            return self._make_request(prompt)
        except Exception as e:
            logger.error(f"QA failed: {e}")
            return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def answer_question_stream(self, question: str, context: str):
        """è³ªå•å¿œç­”æ©Ÿèƒ½ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        prompt = QA_PROMPT.format(
            question=question,
            context=context
        )
        
        try:
            yield from self._make_request_stream(prompt)
        except Exception as e:
            logger.error(f"Streaming QA failed: {e}")
            yield f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def analyze_with_context(self, prompt: str) -> Dict[str, Any]:
        """çµ±åˆåˆ†æç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        try:
            response = self._make_request(prompt)
            return self._extract_and_parse_json(response)
        except Exception as e:
            logger.error(f"Ollama context analysis failed: {e}")
            return None
    
    def _extract_and_parse_json(self, response: str) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã¦ãƒ‘ãƒ¼ã‚¹"""
        try:
            logger.debug(f"LLM response: {response[:200]}...")
            
            # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦è¡Œ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                
                # JSONã®å‰å‡¦ç†ï¼ˆä¸€èˆ¬çš„ãªå•é¡Œã‚’ä¿®æ­£ï¼‰
                json_str = self._clean_json_string(json_str)
                
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as json_e:
                    logger.warning(f"JSON parse failed, attempting repair: {str(json_e)[:100]}")
                    # JSONä¿®å¾©ã‚’è©¦è¡Œ
                    repaired_json = self._repair_json_string(json_str)
                    if repaired_json:
                        try:
                            return json.loads(repaired_json)
                        except json.JSONDecodeError:
                            logger.warning("JSON repair failed")
                            return None
                    return None
            else:
                logger.warning("No JSON found in response")
                return None
                
        except Exception as e:
            logger.error(f"JSON extraction failed: {e}")
            return None
    
    def _fallback_analysis(self, content: str, llm_response: str) -> Dict[str, Any]:
        """JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ†æ
        issues = []
        flags = []
        risk_level = "ä½"
        urgency_score = 1
        
        content_lower = content.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«ã‚ˆã‚‹åˆ†é¡
        if any(word in content_lower for word in ["åå¯¾", "åœæ­¢", "ä¸­æ­¢", "ç·Šæ€¥"]):
            flags.append("emergency_stop")
            risk_level = "é«˜"
            urgency_score = 8
        
        if any(word in content_lower for word in ["é…å»¶", "å»¶æœŸ", "é…ã‚Œ"]):
            flags.append("delay_risk")
            if risk_level == "ä½":
                risk_level = "ä¸­"
                urgency_score = 5
        
        if any(word in content_lower for word in ["ä¸å…·åˆ", "æ•…éšœ", "å•é¡Œ", "ãƒˆãƒ©ãƒ–ãƒ«"]):
            flags.append("technical_issue")
            if risk_level == "ä½":
                risk_level = "ä¸­"
                urgency_score = 6
        
        if any(word in content_lower for word in ["ç”³è«‹", "è¨±å¯", "å…è¨±", "æ‰‹ç¶šã"]):
            flags.append("procedure_problem")
        
        return {
            "project_info": {
                "project_id": "ä¸æ˜",
                "location": "ä¸æ˜", 
                "responsible_person": "ä¸æ˜"
            },
            "status": "åˆ†æä¸­",
            "issues": issues,
            "risk_level": risk_level,
            "recommended_flags": flags,
            "summary": "è©³ç´°åˆ†æãŒå¿…è¦ã§ã™",
            "urgency_score": urgency_score,
            "key_points": ["æ‰‹å‹•ç¢ºèªæ¨å¥¨"]
        }
    
    def _clean_json_string(self, json_str: str) -> str:
        """JSONStringå‰å‡¦ç†ã§ä¸€èˆ¬çš„ãªå•é¡Œã‚’ä¿®æ­£"""
        import re
        
        # æ”¹è¡Œã¨ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’å‰Šé™¤
        json_str = re.sub(r'\n\s*', '', json_str)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤
        json_str = re.sub(r'//.*?(?=\n|$)', '', json_str)
        
        # æœ«å°¾ã‚³ãƒ³ãƒã‚’å‰Šé™¤
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        return json_str.strip()
    
    def _repair_json_string(self, json_str: str) -> str:
        """ç ´æã—ãŸJSONã®ä¿®å¾©ã‚’è©¦è¡Œ"""
        try:
            import re
            
            # ä¸€èˆ¬çš„ãªJSONä¿®å¾©ãƒ‘ã‚¿ãƒ¼ãƒ³
            repairs = [
                # æœªé–‰ã˜ã®å¼•ç”¨ç¬¦ã‚’ä¿®æ­£
                (r'"([^"]*?)(?=[,\}\]])(?<!\\)"', r'"\1"'),
                # æœªé–‰ã˜ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿®æ­£
                (r'([^}])$', r'\1}'),
                # ç©ºã®å€¤ã‚’ä¿®æ­£
                (r':\s*,', r': null,'),
                (r':\s*}', r': null}'),
            ]
            
            repaired = json_str
            for pattern, replacement in repairs:
                repaired = re.sub(pattern, replacement, repaired)
            
            # åŸºæœ¬çš„ãªæ§‹é€ ãƒã‚§ãƒƒã‚¯
            if repaired.count('{') > repaired.count('}'):
                repaired += '}' * (repaired.count('{') - repaired.count('}'))
            
            return repaired
            
        except Exception as e:
            logger.warning(f"JSON repair attempt failed: {e}")
            return None
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçµæœã‚’ä½œæˆ"""
        return {
            "project_info": {
                "project_id": "ã‚¨ãƒ©ãƒ¼",
                "location": "ä¸æ˜",
                "responsible_person": "ä¸æ˜"
            },
            "status": f"åˆ†æã‚¨ãƒ©ãƒ¼: {error_msg}",
            "issues": ["åˆ†æå‡¦ç†å¤±æ•—"],
            "risk_level": "è¦ç¢ºèª",
            "recommended_flags": ["requires_review"],
            "summary": "æ‰‹å‹•ã§ã®ç¢ºèªãŒå¿…è¦ã§ã™",
            "urgency_score": 5,
            "key_points": ["ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šè‡ªå‹•åˆ†æå¤±æ•—"]
        }
    
    def analyze_with_context(self, context_prompt: str) -> Optional[Dict[str, Any]]:
        """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸçµ±åˆåˆ†æï¼ˆæ¡ˆä»¶ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ï¼‰"""
        try:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®å‡¦ç†
            if self.provider == "ollama":
                return self._analyze_with_context_ollama(context_prompt)
            elif self.provider == "openai":
                return self._analyze_with_context_openai(context_prompt)
            elif self.provider == "anthropic":
                return self._analyze_with_context_anthropic(context_prompt)
            else:
                logger.error(f"Unsupported provider for context analysis: {self.provider}")
                return None
                
        except Exception as e:
            logger.error(f"Context analysis failed: {e}")
            return None
    
    def _analyze_with_context_ollama(self, context_prompt: str) -> Optional[Dict[str, Any]]:
        """Ollamaçµ±åˆåˆ†æ"""
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": context_prompt}
                ],
                options={
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 3072,
                    "num_ctx": 16384
                }
            )
            
            content = response['message']['content']
            return self._extract_and_parse_json(content)
            
        except Exception as e:
            logger.error(f"Ollama context analysis failed: {e}")
            return None
    
    def _analyze_with_context_openai(self, context_prompt: str) -> Optional[Dict[str, Any]]:
        """OpenAIçµ±åˆåˆ†æ"""
        try:
            if not ChatOpenAI:
                logger.error("OpenAI not available")
                return None
            
            llm = ChatOpenAI(
                model=self.model,
                api_key=self.api_key,
                temperature=0.1
            )
            
            messages = [
                ("system", SYSTEM_PROMPT),
                ("user", context_prompt)
            ]
            
            response = llm.invoke(messages)
            return self._extract_and_parse_json(response.content)
            
        except Exception as e:
            logger.error(f"OpenAI context analysis failed: {e}")
            return None
    
    def _analyze_with_context_anthropic(self, context_prompt: str) -> Optional[Dict[str, Any]]:
        """Anthropicçµ±åˆåˆ†æ"""
        try:
            if not ChatAnthropic:
                logger.error("Anthropic not available")
                return None
            
            llm = ChatAnthropic(
                model=self.model,
                api_key=self.api_key,
                temperature=0.1
            )
            
            messages = [
                ("system", SYSTEM_PROMPT),
                ("user", context_prompt)
            ]
            
            response = llm.invoke(messages)
            return self._extract_and_parse_json(response.content)
            
        except Exception as e:
            logger.error(f"Anthropic context analysis failed: {e}")
            return None
    
    def answer_question(self, question: str, context: str) -> str:
        """è³ªå•å¿œç­”å‡¦ç†"""
        try:
            if not context or context.strip() == "":
                return "é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # QAãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            qa_prompt = f"""
ä»¥ä¸‹ã®æ–‡è„ˆæƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æ–‡è„ˆæƒ…å ±:
{context}

è³ªå•:
{question}

å›ç­”æŒ‡ç¤º:
- æ–‡è„ˆæƒ…å ±ã«åŸºã¥ã„ã¦å…·ä½“çš„ã«å›ç­”ã—ã¦ãã ã•ã„
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- æ¨æ¸¬ã§ã¯ãªãã€æ–‡è„ˆã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹äº‹å®Ÿã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„
- è¨˜å·ï¼ˆ#ã€*ã€**ãªã©ï¼‰ã¯ä½¿ç”¨ã›ãšã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§å›ç­”ã—ã¦ãã ã•ã„
"""
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®å‡¦ç†
            if self.provider == "ollama":
                return self._answer_with_ollama(qa_prompt)
            elif self.provider == "openai":
                return self._answer_with_openai(qa_prompt)
            elif self.provider == "anthropic":
                return self._answer_with_anthropic(qa_prompt)
            else:
                return f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.provider}"
                
        except Exception as e:
            logger.error(f"Question answering failed: {e}")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def _answer_with_ollama(self, qa_prompt: str) -> str:
        """Ollamaè³ªå•å¿œç­”"""
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å»ºè¨­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸæ–‡è„ˆæƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": qa_prompt}
                ],
                options={
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 1024,
                    "num_ctx": 8192
                }
            )
            
            return response['message']['content']
            
        except Exception as e:
            logger.error(f"Ollama QA failed: {e}")
            return f"Ollamaå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _answer_with_openai(self, qa_prompt: str) -> str:
        """OpenAIè³ªå•å¿œç­”"""
        try:
            if not ChatOpenAI:
                return "OpenAI ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            
            llm = ChatOpenAI(
                model=self.model,
                api_key=self.api_key,
                temperature=0.1
            )
            
            messages = [
                ("system", "ã‚ãªãŸã¯å»ºè¨­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸæ–‡è„ˆæƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"),
                ("user", qa_prompt)
            ]
            
            response = llm.invoke(messages)
            return response.content
            
        except Exception as e:
            logger.error(f"OpenAI QA failed: {e}")
            return f"OpenAIå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _answer_with_anthropic(self, qa_prompt: str) -> str:
        """Anthropicè³ªå•å¿œç­”"""
        try:
            if not ChatAnthropic:
                return "Anthropic ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            
            llm = ChatAnthropic(
                model=self.model,
                api_key=self.api_key,
                temperature=0.1
            )
            
            messages = [
                ("system", "ã‚ãªãŸã¯å»ºè¨­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸæ–‡è„ˆæƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"),
                ("user", qa_prompt)
            ]
            
            response = llm.invoke(messages)
            return response.content
            
        except Exception as e:
            logger.error(f"Anthropic QA failed: {e}")
            return f"Anthropicå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

    def get_provider_info(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’å–å¾—"""
        return {
            "provider": self.provider,
            "model": self.model,
            "status": "connected" if self.model else "disconnected"
        }

# Streamlitç”¨LLMServiceå–å¾—é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
def get_llm_service() -> LLMService:
    """
    LLMServiceã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    
    æ¯å›æ–°ã—ã„LLMServiceã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹åŒä¸€å›ç­”å•é¡Œã‚’å›é¿ã™ã‚‹ã€‚
    """
    logger.info("ğŸ”§ Initializing new LLMService instance...")
    service = LLMService()
    logger.info("âœ… New LLMService instance initialized")
    return service