"""
LLMæ©Ÿèƒ½ã®è‡ªå‹•è©•ä¾¡ã‚µãƒ¼ãƒ“ã‚¹
"""
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

from app.models.report import DocumentReport, FlagType, RiskLevel
from app.config.settings import DATA_DIR

logger = logging.getLogger(__name__)

@dataclass
class EvaluationMetrics:
    """è©•ä¾¡æŒ‡æ¨™"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    confusion_matrix: Dict[str, int]

@dataclass
class EvaluationResult:
    """çµ±åˆåˆ†æè©•ä¾¡çµæœ"""
    report_type_classification: EvaluationMetrics
    status_classification: EvaluationMetrics
    category_classification: EvaluationMetrics
    risk_level_assessment: EvaluationMetrics
    human_review_detection: EvaluationMetrics
    project_mapping: EvaluationMetrics
    overall_score: float

class EvaluationService:
    """LLMæ©Ÿèƒ½è©•ä¾¡ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.ground_truth_path = Path(DATA_DIR) / "evaluation" / "ground_truth.json"
        self.ground_truth = self._load_ground_truth()
    
    def _load_ground_truth(self) -> Dict[str, Any]:
        """æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.ground_truth_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"Ground truth file not found: {self.ground_truth_path}")
            return {}
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse ground truth: {e}")
            return {}
    
    def evaluate_reports(self, reports: List[DocumentReport]) -> EvaluationResult:
        """çµ±åˆåˆ†æçµæœã‚’è©•ä¾¡"""
        if not self.ground_truth:
            raise ValueError("Ground truth data not available")
        
        evaluation_data = self.ground_truth.get("evaluation_data", {})
        
        # ğŸ¯ çµ±åˆåˆ†ææ©Ÿèƒ½ã®è©•ä¾¡
        report_type_metrics = self._evaluate_report_type_classification(reports, evaluation_data)
        status_metrics = self._evaluate_status_classification(reports, evaluation_data)
        category_metrics = self._evaluate_category_classification(reports, evaluation_data)
        risk_metrics = self._evaluate_risk_level_assessment(reports, evaluation_data)
        human_review_metrics = self._evaluate_human_review_detection(reports, evaluation_data)
        project_mapping_metrics = self._evaluate_project_mapping(reports, evaluation_data)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        overall_score = (
            report_type_metrics.f1_score * 0.15 +
            status_metrics.f1_score * 0.25 +
            category_metrics.f1_score * 0.20 +
            risk_metrics.f1_score * 0.20 +
            human_review_metrics.f1_score * 0.10 +
            project_mapping_metrics.f1_score * 0.10
        )
        
        return EvaluationResult(
            report_type_classification=report_type_metrics,
            status_classification=status_metrics,
            category_classification=category_metrics,
            risk_level_assessment=risk_metrics,
            human_review_detection=human_review_metrics,
            project_mapping=project_mapping_metrics,
            overall_score=overall_score
        )
    
    def _evaluate_report_type_classification(self, reports: List[DocumentReport], 
                                           ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ†é¡ã®è©•ä¾¡"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = ground_truth[filename]["expected_report_type"]
                predicted = report.report_type.value if report.report_type else "OTHER"
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_metrics(predictions, actuals, "report_type_classification")
    
    def _evaluate_status_classification(self, reports: List[DocumentReport], 
                                      ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """çŠ¶æ…‹åˆ†é¡ã®è©•ä¾¡ï¼ˆçµ±åˆåˆ†æç‰ˆï¼‰"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = ground_truth[filename]["expected_status"]
                predicted = report.status_flag.value if report.status_flag else "unknown"
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_metrics(predictions, actuals, "status_classification")
    
    def _evaluate_human_review_detection(self, reports: List[DocumentReport], 
                                       ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """åˆ†æå›°é›£æ¤œçŸ¥ã®è©•ä¾¡"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = ground_truth[filename]["expected_requires_human_review"]
                predicted = getattr(report, 'requires_human_review', False)
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_metrics(predictions, actuals, "human_review_detection")
    
    def _evaluate_category_classification(self, reports: List[DocumentReport],
                                        ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã®è©•ä¾¡ï¼ˆçµ±åˆåˆ†æç‰ˆï¼‰"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = set(ground_truth[filename]["expected_categories"])
                predicted = set([cat.value for cat in report.category_labels]) if report.category_labels else set()
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_set_metrics(predictions, actuals, "category_classification")
    
    def _evaluate_risk_level_assessment(self, reports: List[DocumentReport],
                                      ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è©•ä¾¡ã®è©•ä¾¡ï¼ˆçµ±åˆåˆ†æç‰ˆï¼‰"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = ground_truth[filename]["expected_risk_level"]
                predicted = report.risk_level.value if report.risk_level else "ä¸æ˜"
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_metrics(predictions, actuals, "risk_level_assessment")
    
    def _evaluate_project_mapping(self, reports: List[DocumentReport],
                                ground_truth: Dict[str, Any]) -> EvaluationMetrics:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã®è©•ä¾¡"""
        predictions = []
        actuals = []
        
        for report in reports:
            filename = Path(report.file_path).name
            if filename in ground_truth:
                expected = ground_truth[filename]["expected_project_id"]
                predicted = getattr(report, 'project_id', None) or "ä¸æ˜"
                
                predictions.append(predicted)
                actuals.append(expected)
        
        return self._calculate_metrics(predictions, actuals, "project_mapping")
    
    def _calculate_metrics(self, predictions: List[str], actuals: List[str], 
                          metric_name: str) -> EvaluationMetrics:
        """åˆ†é¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        if not predictions or not actuals:
            return EvaluationMetrics(0.0, 0.0, 0.0, 0.0, {})
        
        # æ­£ç¢ºæ€§
        correct = sum(1 for p, a in zip(predictions, actuals) if p == a)
        accuracy = correct / len(predictions)
        
        # å„ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        unique_labels = set(predictions + actuals)
        
        precision_scores = []
        recall_scores = []
        
        confusion_matrix = {}
        
        for label in unique_labels:
            # True Positive, False Positive, False Negative
            tp = sum(1 for p, a in zip(predictions, actuals) if p == label and a == label)
            fp = sum(1 for p, a in zip(predictions, actuals) if p == label and a != label)
            fn = sum(1 for p, a in zip(predictions, actuals) if p != label and a == label)
            
            confusion_matrix[f"{label}_tp"] = tp
            confusion_matrix[f"{label}_fp"] = fp
            confusion_matrix[f"{label}_fn"] = fn
            
            # Precision and Recall
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            
            precision_scores.append(precision)
            recall_scores.append(recall)
        
        # ãƒã‚¯ãƒ­å¹³å‡
        avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0
        avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0
        
        # F1ã‚¹ã‚³ã‚¢
        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) \
                   if (avg_precision + avg_recall) > 0 else 0
        
        return EvaluationMetrics(
            accuracy=accuracy,
            precision=avg_precision,
            recall=avg_recall,
            f1_score=f1_score,
            confusion_matrix=confusion_matrix
        )
    
    def _calculate_set_metrics(self, predictions: List[set], actuals: List[set],
                              metric_name: str) -> EvaluationMetrics:
        """ã‚»ãƒƒãƒˆåˆ†é¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ç”¨ï¼‰"""
        if not predictions or not actuals:
            return EvaluationMetrics(0.0, 0.0, 0.0, 0.0, {})
        
        accuracies = []
        precisions = []
        recalls = []
        
        for pred_set, actual_set in zip(predictions, actuals):
            if not actual_set and not pred_set:
                # ä¸¡æ–¹ç©ºã®å ´åˆã¯å®Œå…¨ä¸€è‡´
                accuracies.append(1.0)
                precisions.append(1.0)
                recalls.append(1.0)
            elif not actual_set:
                # æ­£è§£ãŒç©ºã ãŒäºˆæ¸¬ãŒã‚ã‚‹å ´åˆ
                accuracies.append(0.0)
                precisions.append(0.0)
                recalls.append(1.0)  # ä½•ã‚‚äºˆæ¸¬ã™ã¹ãã§ãªã„ã®ã§ recall ã¯ 1
            elif not pred_set:
                # äºˆæ¸¬ãŒç©ºã ãŒæ­£è§£ãŒã‚ã‚‹å ´åˆ
                accuracies.append(0.0)
                precisions.append(1.0)  # ä½•ã‚‚äºˆæ¸¬ã—ã¦ã„ãªã„ã®ã§ precision ã¯ 1
                recalls.append(0.0)
            else:
                # ä¸¡æ–¹ã«è¦ç´ ãŒã‚ã‚‹å ´åˆ
                intersection = pred_set & actual_set
                union = pred_set | actual_set
                
                accuracy = len(intersection) / len(union) if union else 0
                precision = len(intersection) / len(pred_set) if pred_set else 0
                recall = len(intersection) / len(actual_set) if actual_set else 0
                
                accuracies.append(accuracy)
                precisions.append(precision)
                recalls.append(recall)
        
        avg_accuracy = sum(accuracies) / len(accuracies)
        avg_precision = sum(precisions) / len(precisions)
        avg_recall = sum(recalls) / len(recalls)
        
        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) \
                   if (avg_precision + avg_recall) > 0 else 0
        
        return EvaluationMetrics(
            accuracy=avg_accuracy,
            precision=avg_precision,
            recall=avg_recall,
            f1_score=f1_score,
            confusion_matrix={"set_comparison": len(predictions)}
        )
    
    def _calculate_binary_metrics(self, predictions: List[bool], actuals: List[bool],
                                 metric_name: str) -> EvaluationMetrics:
        """ãƒã‚¤ãƒŠãƒªåˆ†é¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        if not predictions or not actuals:
            return EvaluationMetrics(0.0, 0.0, 0.0, 0.0, {})
        
        tp = sum(1 for p, a in zip(predictions, actuals) if p and a)
        fp = sum(1 for p, a in zip(predictions, actuals) if p and not a)
        fn = sum(1 for p, a in zip(predictions, actuals) if not p and a)
        tn = sum(1 for p, a in zip(predictions, actuals) if not p and not a)
        
        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        confusion_matrix = {
            "true_positive": tp,
            "false_positive": fp,
            "false_negative": fn,
            "true_negative": tn
        }
        
        return EvaluationMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            confusion_matrix=confusion_matrix
        )
    
    def evaluate_project_mapping(self, reports: List[DocumentReport]) -> EvaluationMetrics:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ç²¾åº¦ã‚’è©•ä¾¡"""
        predictions = []
        actuals = []
        mapping_methods = []
        confidence_scores = []
        
        for report in reports:
            file_name = report.file_name
            if file_name in self.ground_truth["evaluation_data"]:
                expected_data = self.ground_truth["evaluation_data"][file_name]
                
                # å®Ÿéš›ã®å€¤
                actual_project_id = expected_data.get("expected_project_id")
                actual_method = expected_data.get("expected_mapping_method")
                
                # äºˆæ¸¬å€¤
                predicted_project_id = getattr(report, 'project_id', None)
                mapping_info = getattr(report, 'project_mapping_info', {})
                predicted_method = mapping_info.get('matching_method', 'unknown')
                confidence = mapping_info.get('confidence_score', 0.0)
                
                if actual_project_id:  # æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è©•ä¾¡
                    predictions.append(predicted_project_id)
                    actuals.append(actual_project_id)
                    mapping_methods.append(predicted_method)
                    confidence_scores.append(confidence)
        
        if not predictions:
            return EvaluationMetrics(0.0, 0.0, 0.0, 0.0, {})
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDä¸€è‡´ç‡
        correct_mappings = sum(1 for p, a in zip(predictions, actuals) if p == a)
        mapping_accuracy = correct_mappings / len(predictions)
        
        # å¹³å‡ä¿¡é ¼åº¦
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        # ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥çµ±è¨ˆ
        method_stats = {}
        for method in set(mapping_methods):
            method_predictions = [p for p, m in zip(predictions, mapping_methods) if m == method]
            method_actuals = [a for a, m in zip(actuals, mapping_methods) if m == method]
            method_confidence = [c for c, m in zip(confidence_scores, mapping_methods) if m == method]
            
            if method_predictions:
                method_correct = sum(1 for p, a in zip(method_predictions, method_actuals) if p == a)
                method_accuracy = method_correct / len(method_predictions)
                method_avg_confidence = sum(method_confidence) / len(method_confidence)
                
                method_stats[method] = {
                    'accuracy': method_accuracy,
                    'count': len(method_predictions),
                    'avg_confidence': method_avg_confidence
                }
        
        return EvaluationMetrics(
            accuracy=mapping_accuracy,
            precision=avg_confidence,  # ä¿¡é ¼åº¦ã‚’precisionã¨ã—ã¦ä½¿ç”¨
            recall=0.0,  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã§ã¯ä½¿ç”¨ã—ãªã„
            f1_score=0.0,  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã§ã¯ä½¿ç”¨ã—ãªã„
            confusion_matrix=method_stats
        )