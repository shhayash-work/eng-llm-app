"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå°‚ç”¨ãƒ™ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
è»½é‡ãƒ»é«˜é€Ÿãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªå®Ÿè£…
"""

import json
import pickle
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import numpy as np
import ollama
import logging

logger = logging.getLogger(__name__)

@dataclass
class ProjectVectorInfo:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æƒ…å ±"""
    project_id: str
    project_name: str
    location: str
    embedding: List[float]
    metadata: Dict[str, str]

@dataclass
class VectorSearchResult:
    """ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢çµæœ"""
    project_id: str
    similarity_score: float
    matched_keywords: List[str]

class ProjectVectorMapper:
    """è»½é‡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°"""
    
    def __init__(self):
        self.cache_dir = Path("data/vector_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        self.vector_cache_file = self.cache_dir / "project_vectors.pkl"
        self.metadata_cache_file = self.cache_dir / "project_metadata.json"
        
        self.ollama_client = ollama.Client()
        self.embedding_model = "mxbai-embed-large:latest"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰
        self.project_vectors = self._load_vector_cache()
        self.project_metadata = self._load_metadata_cache()
        
        logger.info(f"ProjectVectorMapper initialized: {len(self.project_vectors)} projects loaded")
    
    def _load_vector_cache(self) -> Dict[str, np.ndarray]:
        """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰"""
        if self.vector_cache_file.exists():
            try:
                with open(self.vector_cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Failed to load vector cache: {e}")
        return {}
    
    def _save_vector_cache(self):
        """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
        try:
            with open(self.vector_cache_file, 'wb') as f:
                pickle.dump(self.project_vectors, f)
            logger.info("Vector cache saved")
        except Exception as e:
            logger.error(f"Failed to save vector cache: {e}")
    
    def _load_metadata_cache(self) -> Dict[str, Dict]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰"""
        if self.metadata_cache_file.exists():
            try:
                with open(self.metadata_cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load metadata cache: {e}")
        return {}
    
    def _save_metadata_cache(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
        try:
            with open(self.metadata_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.project_metadata, f, ensure_ascii=False, indent=2)
            logger.info("Metadata cache saved")
        except Exception as e:
            logger.error(f"Failed to save metadata cache: {e}")
    
    def add_project(self, project_info: Dict[str, str]) -> bool:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
        
        Args:
            project_info: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ± (project_id, project_name, location, etc.)
        """
        try:
            project_id = project_info['project_id']
            
            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            if project_id in self.project_vectors:
                logger.info(f"Project {project_id} already exists, skipping")
                return True
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜è¿°æ–‡ä½œæˆ
            description = self._create_project_description(project_info)
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            start_time = time.time()
            response = self.ollama_client.embeddings(
                model=self.embedding_model,
                prompt=description
            )
            embedding_time = time.time() - start_time
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼ä¿å­˜
            self.project_vectors[project_id] = np.array(response['embedding'])
            self.project_metadata[project_id] = {
                **project_info,
                'description': description,
                'embedding_time': embedding_time,
                'added_at': datetime.now().isoformat()
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆãƒãƒƒãƒå‡¦ç†å‘ã‘ï¼‰
            if len(self.project_vectors) % 100 == 0:  # 100ä»¶ã”ã¨ã«ä¿å­˜
                self._save_vector_cache()
                self._save_metadata_cache()
            
            logger.info(f"Project {project_id} added (embedding: {embedding_time:.3f}s)")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add project {project_info.get('project_id', 'unknown')}: {e}")
            return False
    
    def _create_project_description(self, project_info: Dict[str, str]) -> str:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜è¿°æ–‡ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰"""
        parts = []
        
        # åŸºæœ¬æƒ…å ±
        if project_info.get('project_name'):
            parts.append(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project_info['project_name']}")
        
        if project_info.get('station_name'):
            parts.append(f"å±€å: {project_info['station_name']}")
        
        if project_info.get('station_number'):
            parts.append(f"å±€ç•ª: {project_info['station_number']}")
        
        if project_info.get('location'):
            parts.append(f"å ´æ‰€: {project_info['location']}")
        
        if project_info.get('aurora_plan'):
            parts.append(f"Auroraè¨ˆç”»: {project_info['aurora_plan']}")
        
        if project_info.get('responsible_person'):
            parts.append(f"æ‹…å½“è€…: {project_info['responsible_person']}")
        
        if project_info.get('current_phase'):
            parts.append(f"ç¾åœ¨ãƒ•ã‚§ãƒ¼ã‚º: {project_info['current_phase']}")
        
        return " ".join(parts)
    
    def search_similar_projects(
        self, 
        query_text: str, 
        top_k: int = 5,
        similarity_threshold: float = 0.3
    ) -> List[VectorSearchResult]:
        """
        é¡ä¼¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œç´¢
        
        Args:
            query_text: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: ä¸Šä½ä½•ä»¶å–å¾—ã™ã‚‹ã‹
            similarity_threshold: é¡ä¼¼åº¦ã®æœ€ä½é–¾å€¤
        """
        try:
            if not self.project_vectors:
                logger.warning("No project vectors available")
                return []
            
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            response = self.ollama_client.embeddings(
                model=self.embedding_model,
                prompt=query_text
            )
            query_vector = np.array(response['embedding'])
            
            # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã®é¡ä¼¼åº¦è¨ˆç®—
            similarities = []
            for project_id, project_vector in self.project_vectors.items():
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarity = self._cosine_similarity(query_vector, project_vector)
                
                if similarity >= similarity_threshold:
                    similarities.append((project_id, similarity))
            
            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # çµæœä½œæˆ
            results = []
            for project_id, similarity in similarities[:top_k]:
                metadata = self.project_metadata.get(project_id, {})
                matched_keywords = self._extract_matched_keywords(
                    query_text, 
                    metadata.get('description', '')
                )
                
                results.append(VectorSearchResult(
                    project_id=project_id,
                    similarity_score=similarity,
                    matched_keywords=matched_keywords
                ))
            
            logger.info(f"Vector search completed: {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
        except Exception:
            return 0.0
    
    def _extract_matched_keywords(self, query: str, description: str) -> List[str]:
        """ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼‰"""
        import re
        from difflib import SequenceMatcher
        
        query_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ a-zA-Z0-9]+', query))
        desc_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ a-zA-Z0-9]+', description))
        
        # å®Œå…¨ä¸€è‡´
        exact_matched = query_words.intersection(desc_words)
        
        # è¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼ˆé¡ä¼¼åº¦0.8ä»¥ä¸Šï¼‰
        fuzzy_matched = []
        for q_word in query_words:
            for d_word in desc_words:
                if q_word not in exact_matched and d_word not in exact_matched:
                    similarity = SequenceMatcher(None, q_word, d_word).ratio()
                    if similarity >= 0.8:
                        fuzzy_matched.append(f"{q_word}â‰ˆ{d_word}")
        
        return list(exact_matched) + fuzzy_matched
    
    def generate_search_reasoning(self, query_text: str, search_results: List[VectorSearchResult]) -> Dict[str, Any]:
        """ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢çµæœã®è©³ç´°ãªæ ¹æ‹ ç”Ÿæˆï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼‰"""
        if not search_results:
            return {
                "method": "vector_search",
                "status": "no_results",
                "reason": "è©²å½“ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                "confidence": 0.0
            }
        
        top_result = search_results[0]
        project_metadata = self.project_metadata.get(top_result.project_id, {})
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãªæ ¹æ‹ åˆ†æ
        reasoning_details = {
            "method": "vector_search_with_fuzzy",
            "vector_similarity": top_result.similarity_score,
            "project_id": top_result.project_id,
            "project_name": project_metadata.get('project_name', 'ä¸æ˜'),
            "matched_elements": [],
            "fuzzy_matches": [],
            "confidence": top_result.similarity_score
        }
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¿ãƒ¼ã®å„é …ç›®ã¨ã®ä¸€è‡´åº¦åˆ†æ
        master_fields = {
            "station_name": "å±€å",
            "station_number": "å±€ç•ª", 
            "location": "æ‰€åœ¨åœ°",
            "aurora_plan": "Auroraãƒ—ãƒ©ãƒ³",
            "responsible_person": "æ‹…å½“è€…"
        }
        
        from difflib import SequenceMatcher
        import re
        
        query_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ a-zA-Z0-9\-]+', query_text))
        
        for field_key, field_name in master_fields.items():
            field_value = project_metadata.get(field_key, '')
            if not field_value or field_value == 'ä¸æ˜':
                continue
                
            # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            if field_value in query_text:
                reasoning_details["matched_elements"].append({
                    "type": field_name,
                    "master_value": field_value,
                    "match_type": "å®Œå…¨ä¸€è‡´",
                    "similarity": 1.0
                })
                continue
            
            # éƒ¨åˆ†ä¸€è‡´ãƒ»è¡¨è¨˜ã‚†ã‚Œãƒã‚§ãƒƒã‚¯
            field_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ a-zA-Z0-9\-]+', field_value))
            
            for field_word in field_words:
                if len(field_word) < 2:  # çŸ­ã™ãã‚‹å˜èªã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                    
                # å®Œå…¨ä¸€è‡´
                if field_word in query_words:
                    reasoning_details["matched_elements"].append({
                        "type": field_name,
                        "master_value": field_word,
                        "match_type": "éƒ¨åˆ†ä¸€è‡´",
                        "similarity": 1.0
                    })
                    continue
                
                # è¡¨è¨˜ã‚†ã‚Œãƒã‚§ãƒƒã‚¯
                for query_word in query_words:
                    if len(query_word) < 2:
                        continue
                    similarity = SequenceMatcher(None, field_word, query_word).ratio()
                    if similarity >= 0.8:
                        reasoning_details["fuzzy_matches"].append({
                            "type": field_name,
                            "master_value": field_word,
                            "query_value": query_word,
                            "similarity": similarity,
                            "match_type": "è¡¨è¨˜ã‚†ã‚Œ"
                        })
        
        # ç·åˆçš„ãªæ ¹æ‹ æ–‡ç”Ÿæˆ
        reason_parts = []
        
        if reasoning_details["matched_elements"]:
            exact_matches = [m for m in reasoning_details["matched_elements"] if m["match_type"] in ["å®Œå…¨ä¸€è‡´", "éƒ¨åˆ†ä¸€è‡´"]]
            if exact_matches:
                match_descriptions = [f"{m['type']}ã€Œ{m['master_value']}ã€" for m in exact_matches]
                reason_parts.append(f"å®Œå…¨ä¸€è‡´: {', '.join(match_descriptions)}")
        
        if reasoning_details["fuzzy_matches"]:
            fuzzy_descriptions = [f"{m['type']}ã€Œ{m['master_value']}ã€â‰ˆã€Œ{m['query_value']}ã€({m['similarity']:.2f})" for m in reasoning_details["fuzzy_matches"]]
            reason_parts.append(f"è¡¨è¨˜ã‚†ã‚Œ: {', '.join(fuzzy_descriptions)}")
        
        if reason_parts:
            reasoning_details["reason"] = f"ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ã¯{top_result.similarity_score:.3f}ã¨ä¸€ç•ªé«˜ãã€{'; '.join(reason_parts)}ãŒä¸€è‡´ï¼é¡ä¼¼ã—ã¦ã„ã‚‹ãŸã‚ã§ã™"
        else:
            reasoning_details["reason"] = f"ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ã¯{top_result.similarity_score:.3f}ã¨ä¸€ç•ªé«˜ã„ã§ã™ãŒã€æ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ã¯ã‚ã‚Šã¾ã›ã‚“"
            reasoning_details["confidence"] = min(reasoning_details["confidence"], 0.6)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãŒãªã„å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
        
        # ä¿¡é ¼åº¦èª¿æ•´ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªå€¤ã«ï¼‰
        match_count = len(reasoning_details["matched_elements"]) + len(reasoning_details["fuzzy_matches"])
        if match_count > 0:
            # ãƒãƒƒãƒæ•°ã«å¿œã˜ã¦ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹ãŒã€ãƒ™ã‚¯ã‚¿ãƒ¼é¡ä¼¼åº¦ã‚’åŸºæº–ã¨ã™ã‚‹
            confidence_boost = match_count * 0.05  # ã‚ˆã‚Šæ§ãˆã‚ãªèª¿æ•´
            reasoning_details["confidence"] = min(reasoning_details["confidence"] + confidence_boost, 0.95)  # æœ€å¤§0.95ã«åˆ¶é™
        else:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãŒãªã„å ´åˆã¯ãƒ™ã‚¯ã‚¿ãƒ¼é¡ä¼¼åº¦ã®ã¿
            reasoning_details["confidence"] = reasoning_details["confidence"]
        
        return reasoning_details
    
    def update_project_vectors_from_master(self) -> int:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
            master_file = Path("data/sample_construction_data/project_reports_mapping.json")
            if not master_file.exists():
                logger.error("Project master file not found")
                return 0
            
            with open(master_file, 'r', encoding='utf-8') as f:
                projects = json.load(f)
            
            added_count = 0
            for project in projects:
                if self.add_project(project):
                    added_count += 1
            
            # æœ€çµ‚ä¿å­˜
            self._save_vector_cache()
            self._save_metadata_cache()
            
            logger.info(f"Vector update completed: {added_count} new projects added")
            return added_count
            
        except Exception as e:
            logger.error(f"Failed to update project vectors: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "total_projects": len(self.project_vectors),
            "cache_files": {
                "vectors": self.vector_cache_file.exists(),
                "metadata": self.metadata_cache_file.exists()
            },
            "embedding_model": self.embedding_model
        }

# ä½¿ç”¨ä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ã®è£œåŠ©é–¢æ•°
def benchmark_vector_operations():
    """ãƒ™ã‚¯ã‚¿ãƒ¼æ“ä½œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    import time
    
    mapper = ProjectVectorMapper()
    
    # åˆæœŸåŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    start_time = time.time()
    added_count = mapper.update_project_vectors_from_master()
    init_time = time.time() - start_time
    
    print(f"ğŸš€ åˆæœŸåŒ–æ™‚é–“: {init_time:.3f}ç§’ ({added_count}ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)")
    
    # æ¤œç´¢ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    test_queries = [
        "æ¨ªæµœå¸‚æ¸¯åŒ—åŒº 5GåŸºåœ°å±€",
        "åƒè‘‰çœŒå¸‚å·å¸‚ ã‚¢ãƒ³ãƒ†ãƒŠå·¥äº‹",
        "åŸ¼ç‰çœŒå·å£å¸‚ åŸºåœ°å±€å»ºè¨­"
    ]
    
    for query in test_queries:
        start_time = time.time()
        results = mapper.search_similar_projects(query, top_k=3)
        search_time = time.time() - start_time
        
        print(f"ğŸ” æ¤œç´¢: '{query}' - {search_time:.3f}ç§’ ({len(results)}ä»¶)")
        for result in results:
            print(f"   â†’ {result.project_id} (é¡ä¼¼åº¦: {result.similarity_score:.3f})")

if __name__ == "__main__":
    benchmark_vector_operations()