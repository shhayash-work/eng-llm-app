"""
åŠ¹ç‡çš„ãªRAGå‡¦ç†ã‚’å®Ÿè£…ã—ãŸåˆ†æãƒ‘ãƒãƒ«UI
"""
import streamlit as st
import json
from typing import List, Dict, Any
from pathlib import Path

from app.models.report import DocumentReport
from app.services.llm_service import get_llm_service
from app.services.vector_store import VectorStoreService

def load_all_processed_reports() -> Dict[str, List[DocumentReport]]:
    """å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸ã‚’æ¡ˆä»¶IDåˆ¥ã«èª­ã¿è¾¼ã¿"""
    reports_by_project = {}
    processed_dir = Path("data/processed_reports")
    
    if not processed_dir.exists():
        return {}
    
    for report_file in processed_dir.glob("*.json"):
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                report_data = json.load(f)
            
            project_id = report_data.get('project_id')
            if project_id:
                if project_id not in reports_by_project:
                    reports_by_project[project_id] = []
                
                # DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                report = type('Report', (), {
                    'file_name': report_data.get('file_name', ''),
                    'project_id': project_id,
                    'content': report_data.get('content_preview', ''),
                    'analysis_result': type('Analysis', (), {
                        'summary': report_data.get('analysis_result', {}).get('summary', ''),
                        'issues': report_data.get('analysis_result', {}).get('issues', []),
                        'key_points': report_data.get('analysis_result', {}).get('key_points', [])
                    })(),
                    'risk_level': report_data.get('risk_level', 'ä¸æ˜'),
                    'status_flag': report_data.get('status_flag', 'ä¸æ˜')
                })()
                
                reports_by_project[project_id].append(report)
                
        except Exception as e:
            st.warning(f"å ±å‘Šæ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {report_file.name} - {e}")
    
    return reports_by_project

def process_qa_question_efficient(question: str, reports: List[DocumentReport]) -> str:
    """åŠ¹ç‡çš„ãªRAGå‡¦ç†ã«ã‚ˆã‚‹è³ªå•å¿œç­”"""
    try:
        vector_store = VectorStoreService()
        
        # ğŸ” Step 1: çµ±åˆåˆ†æçµæœã‹ã‚‰é–¢é€£æ¡ˆä»¶ã‚’æ¤œç´¢
        context_results = vector_store.search_similar_documents(
            query=question,
            n_results=5,
            filter_metadata={'type': 'context_analysis'}  # çµ±åˆåˆ†æçµæœã®ã¿æ¤œç´¢
        )
        
        if not context_results:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢
            return _fallback_search(question, reports, vector_store)
        
        # ğŸ¯ Step 2: é–¢é€£æ¡ˆä»¶IDã‚’ç‰¹å®š
        related_project_ids = []
        context_parts = []
        
        for result in context_results:
            similarity_score = 1 - result.get('distance', 0.0)
            if similarity_score > 0.3:  # é¡ä¼¼åº¦é–¾å€¤
                metadata = result.get('metadata', {})
                project_id = metadata.get('project_id')
                
                if project_id and project_id not in related_project_ids:
                    related_project_ids.append(project_id)
                
                # çµ±åˆåˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
                context_parts.append(
                    f"=== æ¡ˆä»¶çµ±åˆåˆ†æçµæœ ({project_id}) ===\\n"
                    f"é¡ä¼¼åº¦: {similarity_score:.3f}\\n"
                    f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {metadata.get('overall_status', 'ä¸æ˜')}\\n"
                    f"ç·åˆãƒªã‚¹ã‚¯: {metadata.get('overall_risk', 'ä¸æ˜')}\\n"
                    f"ç¾åœ¨å·¥ç¨‹: {metadata.get('current_phase', 'ä¸æ˜')}\\n"
                    f"é€²æ—å‚¾å‘: {metadata.get('progress_trend', 'ä¸æ˜')}\\n"
                    f"å†…å®¹: {result.get('content', '')[:300]}...\\n"
                )
        
        # ğŸ“„ Step 3: é–¢é€£æ¡ˆä»¶ã®å…¨å ±å‘Šæ›¸ã‚’å–å¾—
        reports_by_project = load_all_processed_reports()
        
        for project_id in related_project_ids[:3]:  # ä¸Šä½3æ¡ˆä»¶
            if project_id in reports_by_project:
                project_reports = reports_by_project[project_id]
                context_parts.append(f"\\n=== æ¡ˆä»¶ {project_id} ã®é–¢é€£å ±å‘Šæ›¸ ===")
                
                for i, report in enumerate(project_reports[:3]):  # æ¡ˆä»¶ã‚ãŸã‚Šä¸Šä½3ä»¶
                    context_parts.append(
                        f"å ±å‘Šæ›¸{i+1}: {report.file_name}\\n"
                        f"è¦ç´„: {report.analysis_result.summary}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {report.risk_level}\\n"
                        f"å•é¡Œ: {', '.join(report.analysis_result.issues)}\\n"
                    )
        
        # ğŸ¤– Step 4: LLMã«è³ªå•
        context = "\\n".join(context_parts)
        llm_service = get_llm_service()
        answer = llm_service.answer_question(question, context)
        
        return answer
        
    except Exception as e:
        return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def _fallback_search(question: str, reports: List[DocumentReport], vector_store: VectorStoreService) -> str:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢"""
    try:
        # é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢
        search_results = vector_store.search_similar_documents(
            query=question,
            n_results=8,
            filter_metadata={'type': {'$ne': 'context_analysis'}}  # çµ±åˆåˆ†æçµæœä»¥å¤–
        )
        
        context_parts = []
        for i, result in enumerate(search_results):
            similarity_score = 1 - result.get('distance', 0.0)
            if similarity_score > 0.3:
                metadata = result.get('metadata', {})
                content = result.get('content', '')
                
                context_parts.append(
                    f"é–¢é€£æ–‡æ›¸{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}):\\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                    f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                    f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                    f"å†…å®¹: {content[:300]}...\\n"
                )
        
        context = "\\n".join(context_parts)
        llm_service = get_llm_service()
        return llm_service.answer_question(question, context)
        
    except Exception as e:
        return f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def render_efficient_qa_interface(reports: List[DocumentReport], use_streaming: bool = True):
    """åŠ¹ç‡çš„ãªRAGå‡¦ç†ã‚’ä½¿ç”¨ã—ãŸè³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.markdown("<div class='custom-header'>å»ºè¨­å·¥ç¨‹ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ï¼ˆåŠ¹ç‡çš„RAGï¼‰</div>", unsafe_allow_html=True)
    st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>çµ±åˆåˆ†æçµæœâ†’é–¢é€£æ¡ˆä»¶ã®å ±å‘Šæ›¸ã‚’åŠ¹ç‡çš„ã«æ¤œç´¢ã—ã¦AIãŒå›ç­”</p>", unsafe_allow_html=True)
    
    # è³ªå•å…¥åŠ›
    question = st.text_area(
        "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: MO0005ã®é€²æ—çŠ¶æ³ã¯ã©ã†ã§ã™ã‹ï¼Ÿ\nä¾‹: é…å»¶ãŒç™ºç”Ÿã—ã¦ã„ã‚‹æ¡ˆä»¶ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\nä¾‹: ã‚ªãƒ¼ãƒŠãƒ¼äº¤æ¸‰ã§å•é¡ŒãŒèµ·ãã¦ã„ã‚‹æ¡ˆä»¶ã‚’æ•™ãˆã¦",
        height=100
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("ğŸ” è³ªå•ã™ã‚‹", type="primary", use_container_width=True):
            if question.strip():
                with st.spinner("åŠ¹ç‡çš„RAGå‡¦ç†ã§å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                    answer = process_qa_question_efficient(question, reports)
                
                st.markdown("### ğŸ’¡ å›ç­”")
                st.info(answer)
                st.success("âœ… åŠ¹ç‡çš„RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
            else:
                st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    with col2:
        st.markdown("**ğŸ’¡ åŠ¹ç‡çš„RAGå‡¦ç†ã®æµã‚Œ:**")
        st.markdown("""
        1. ğŸ” çµ±åˆåˆ†æçµæœã‹ã‚‰é–¢é€£æ¡ˆä»¶ã‚’æ¤œç´¢
        2. ğŸ¯ é–¢é€£æ¡ˆä»¶IDã‚’ç‰¹å®š
        3. ğŸ“„ é–¢é€£æ¡ˆä»¶ã®å…¨å ±å‘Šæ›¸ã‚’å–å¾—
        4. ğŸ¤– çµ±åˆåˆ†æçµæœï¼‹é–¢é€£å ±å‘Šæ›¸ã§LLMå›ç­”
        """)