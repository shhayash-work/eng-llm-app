"""
åˆ†æãƒ‘ãƒãƒ«UI
"""
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict, Any
import pandas as pd
from datetime import datetime, timedelta

from app.models.report import DocumentReport, FlagType
from app.services.llm_service import get_llm_service
from app.services.vector_store import VectorStoreService

def render_analysis_panel(reports: List[DocumentReport]):
    """åˆ†æãƒ‘ãƒãƒ«ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>AIå¯¾è©±åˆ†æ</div>", unsafe_allow_html=True)
    st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>RAGæŠ€è¡“ã«ã‚ˆã‚‹å ±å‘Šæ›¸æ¤œç´¢ã¨LLMã«ã‚ˆã‚‹è‡ªç„¶è¨€èªã§ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </p>", unsafe_allow_html=True)
    
    # è¡¨ç¤ºè¨­å®šï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸Šéƒ¨ã«å·¦å¯„ã›é…ç½®ï¼‰
    st.markdown("**è¡¨ç¤ºè¨­å®š**")
    col_s1, col_s2, col_spacer = st.columns([2, 2, 6])
    with col_s1:
        use_streaming = st.checkbox("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º", value=True, help="å›ç­”ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º")
    with col_s2:
        show_thinking = st.checkbox("æ€è€ƒéç¨‹è¡¨ç¤º", value=False, help="AIã®æ€è€ƒéç¨‹ã‚’è¡¨ç¤º")
    
    st.divider()
    
    # è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    render_qa_interface(reports, use_streaming, show_thinking)

def render_qa_interface(reports: List[DocumentReport], use_streaming: bool = True, show_thinking: bool = False):
    """è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>å»ºè¨­å·¥ç¨‹ã«ã¤ã„ã¦è³ªå•ã™ã‚‹</div>", unsafe_allow_html=True)
    st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>è‡ªç„¶è¨€èªã§å»ºè¨­å·¥ç¨‹ã‚„æ¡ˆä»¶çŠ¶æ³ã«ã¤ã„ã¦è³ªå•ã—ã€é–¢é€£å ±å‘Šæ›¸ã‚’æ¤œç´¢ã—ã¦AIãŒå›ç­”</p>", unsafe_allow_html=True)
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.write("**ã‚µãƒ³ãƒ—ãƒ«è³ªå•:**")
    sample_questions = [
        "ç¾åœ¨é€²è¡Œä¸­ã®ãƒˆãƒ©ãƒ–ãƒ«æ¡ˆä»¶ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æœ€ã‚‚ç·Šæ€¥åº¦ã®é«˜ã„æ¡ˆä»¶ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ä½æ°‘åå¯¾ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ç¾å ´ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "å·¥æœŸé…å»¶ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹æ¡ˆä»¶ã‚’æ•™ãˆã¦ãã ã•ã„",
        "è¨­å‚™ä¸å…·åˆãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ç¾å ´ã¯ã©ã“ã§ã™ã‹ï¼Ÿ"
    ]
    
    selected_question = st.selectbox(
        "ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠï¼ˆã¾ãŸã¯ä¸‹ã«ç‹¬è‡ªã®è³ªå•ã‚’å…¥åŠ›ï¼‰",
        ["è³ªå•ã‚’é¸æŠ..."] + sample_questions
    )
    
    # è³ªå•å…¥åŠ›
    if selected_question != "è³ªå•ã‚’é¸æŠ...":
        question = st.text_input("è³ªå•å†…å®¹:", value=selected_question)
    else:
        question = st.text_input("è³ªå•å†…å®¹:")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªAIè³ªå•ãƒœã‚¿ãƒ³ï¼ˆè¨­å®šãªã—ï¼‰
    ask_button = st.button("AIã«è³ªå•ã™ã‚‹", type="primary", use_container_width=True)
    
    if ask_button:
        if question:
            st.write("**ğŸ¤– RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹AIå›ç­”:**")
            
            # RAGã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œå¯è¦–åŒ–
            with st.spinner("ğŸ” é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ä¸­..."):
                # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®å®Ÿè¡Œã¨çµæœè¡¨ç¤º
                vector_store = VectorStoreService()
                search_results = vector_store.search_similar_documents(
                    query=question, 
                    n_results=8
                )
                
                # æ¤œç´¢çµæœã®å¯è¦–åŒ–
                if search_results:
                    relevant_docs = [r for r in search_results if (1 - r.get('distance', 0.0)) > 0.3]
                    if relevant_docs:
                        st.success(f"âœ… {len(relevant_docs)}ä»¶ã®é–¢é€£æ–‡æ›¸ã‚’ç™ºè¦‹")
                        
                        # æ¤œç´¢çµæœã®è©³ç´°è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if show_thinking:
                            with st.expander("ğŸ” æ¤œç´¢ã•ã‚ŒãŸé–¢é€£æ–‡æ›¸"):
                                for i, result in enumerate(relevant_docs[:3]):
                                    similarity = 1 - result.get('distance', 0.0)
                                    metadata = result.get('metadata', {})
                                    st.write(f"**{i+1}. {metadata.get('file_name', 'ä¸æ˜')}** (é¡ä¼¼åº¦: {similarity:.3f})")
                                    st.write(f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}")
                                    st.write(f"å†…å®¹æŠœç²‹: {result.get('content', '')[:150]}...")
                                    st.divider()
                    else:
                        st.warning("âš ï¸ é–¢é€£åº¦ã®é«˜ã„æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨")
                else:
                    st.warning("âš ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨")
            
            # æ€è€ƒéç¨‹è¡¨ç¤º
            if show_thinking:
                with st.spinner("ğŸ§  AIãŒæ–‡æ›¸ã‚’åˆ†æä¸­..."):
                    import time
                    time.sleep(1)  # æ€è€ƒæ¼”å‡º
                st.success("ğŸ’¡ å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™")
            
            # çµ±ä¸€ã•ã‚ŒãŸå›ç­”è¡¨ç¤ºã‚³ãƒ³ãƒ†ãƒŠï¼ˆå…ƒã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
            response_placeholder = st.empty()
            
            if use_streaming:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºï¼ˆå…ƒã®infoé¢¨ã‚¹ã‚¿ã‚¤ãƒ«å†…ã§ï¼‰
                full_response = ""
                chunk_count = 0
                
                for chunk in process_qa_question_stream(question, reports):
                    full_response += chunk
                    chunk_count += 1
                    
                    # 3æ–‡å­—ã”ã¨ã«æ›´æ–°ï¼ˆå…ƒã®infoé¢¨ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰
                    if chunk_count % 3 == 0:
                        with response_placeholder.container():
                            st.info(f"{full_response}â–Œ")  # ã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ã‚½ãƒ«ä»˜ã
                
                # æœ€çµ‚è¡¨ç¤ºï¼ˆã‚«ãƒ¼ã‚½ãƒ«å‰Šé™¤ï¼‰
                with response_placeholder.container():
                    st.info(full_response)
                
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
                
            else:
                # å¾“æ¥ã®ä¸€æ‹¬è¡¨ç¤ºï¼ˆå…ƒã®ã‚¹ã‚¿ã‚¤ãƒ«ç¶­æŒï¼‰
                if show_thinking:
                    with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = process_qa_question(question, reports)
                else:
                    with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = process_qa_question(question, reports)
                
                # å…ƒã®ã‚·ãƒ³ãƒ—ãƒ«ãªinfoè¡¨ç¤º
                with response_placeholder.container():
                    st.info(answer)
                
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def process_qa_question(question: str, reports: List[DocumentReport]) -> str:
    """è³ªå•å¿œç­”ã‚’å‡¦ç†ï¼ˆRAGã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
    try:
        # ğŸ” RAGã‚·ã‚¹ãƒ†ãƒ : è³ªå•å†…å®¹ã«åŸºã¥ã„ã¦é–¢é€£æ–‡æ›¸ã‚’å‹•çš„æ¤œç´¢
        vector_store = VectorStoreService()
        search_results = vector_store.search_similar_documents(
            query=question, 
            n_results=8  # ã‚ˆã‚Šå¤šãã®é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        )
        
        # æ¤œç´¢çµæœã‹ã‚‰é«˜å“è³ªãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context_parts = []
        
        if search_results:
            for i, result in enumerate(search_results):
                similarity_score = 1 - result.get('distance', 0.0)
                if similarity_score > 0.3:  # é¡ä¼¼åº¦é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    metadata = result.get('metadata', {})
                    content = result.get('content', '')
                    
                    context_parts.append(
                        f"é–¢é€£æ–‡æ›¸{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}):\\n"
                        f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                        f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                        f"å†…å®¹: {content[:300]}...\\n"
                    )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã§çµæœãŒå°‘ãªã„å ´åˆã¯æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚‚è¿½åŠ 
        if len(context_parts) < 3:
            for i, report in enumerate(reports[:5]):
                if report.analysis_result:
                    context_parts.append(
                        f"æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆ{i+1}: {report.file_name}\\n"
                        f"è¦ç´„: {report.analysis_result.summary}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {getattr(report, 'risk_level', 'ä¸æ˜')}\\n"
                        f"å•é¡Œ: {', '.join(report.analysis_result.issues)}\\n"
                    )
        
        context = "\\n".join(context_parts)
        
        # LLMã«è³ªå•ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
        llm_service = get_llm_service()
        answer = llm_service.answer_question(question, context)
        
        return answer
        
    except Exception as e:
        return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def process_qa_question_stream(question: str, reports: List[DocumentReport]):
    """è³ªå•å¿œç­”ã‚’å‡¦ç†ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œãƒ»RAGã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
    try:
        # ğŸ” RAGã‚·ã‚¹ãƒ†ãƒ : è³ªå•å†…å®¹ã«åŸºã¥ã„ã¦é–¢é€£æ–‡æ›¸ã‚’å‹•çš„æ¤œç´¢
        vector_store = VectorStoreService()
        search_results = vector_store.search_similar_documents(
            query=question, 
            n_results=8  # ã‚ˆã‚Šå¤šãã®é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        )
        
        # æ¤œç´¢çµæœã‹ã‚‰é«˜å“è³ªãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context_parts = []
        
        if search_results:
            for i, result in enumerate(search_results):
                similarity_score = 1 - result.get('distance', 0.0)
                if similarity_score > 0.3:  # é¡ä¼¼åº¦é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    metadata = result.get('metadata', {})
                    content = result.get('content', '')
                    
                    context_parts.append(
                        f"é–¢é€£æ–‡æ›¸{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}):\\n"
                        f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                        f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                        f"å†…å®¹: {content[:300]}...\\n"
                    )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã§çµæœãŒå°‘ãªã„å ´åˆã¯æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚‚è¿½åŠ 
        if len(context_parts) < 3:
            for i, report in enumerate(reports[:5]):
                if report.analysis_result:
                    context_parts.append(
                        f"æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆ{i+1}: {report.file_name}\\n"
                        f"è¦ç´„: {report.analysis_result.summary}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {getattr(report, 'risk_level', 'ä¸æ˜')}\\n"
                        f"å•é¡Œ: {', '.join(report.analysis_result.issues)}\\n"
                    )
        
        context = "\\n".join(context_parts)
        
        # LLMã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è³ªå•ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
        llm_service = get_llm_service()
        yield from llm_service.answer_question_stream(question, context)
        
    except Exception as e:
        yield f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def render_similarity_search():
    """é¡ä¼¼ã‚±ãƒ¼ã‚¹æ¤œç´¢ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>é¡ä¼¼ã‚±ãƒ¼ã‚¹æ¤œç´¢</div>", unsafe_allow_html=True)
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›
    search_query = st.text_input(
        "æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: ä½æ°‘åå¯¾ã«ã‚ˆã‚‹å·¥äº‹åœæ­¢"
    )
    
    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    col1, col2 = st.columns(2)
    with col1:
        max_results = st.slider("æœ€å¤§è¡¨ç¤ºä»¶æ•°", 1, 20, 5)
    with col2:
        similarity_threshold = st.slider("é¡ä¼¼åº¦é–¾å€¤", 0.0, 1.0, 0.5)
    
    if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ"):
        if search_query:
            with st.spinner("é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­..."):
                results = search_similar_cases(search_query, max_results)
                
                if results:
                    st.write(f"**{len(results)}ä»¶ã®é¡ä¼¼ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:**")
                    
                    for i, result in enumerate(results, 1):
                        with st.expander(f"{i}. {result['metadata'].get('file_name', 'ä¸æ˜')} (é¡ä¼¼åº¦: {1-result['distance']:.3f})"):
                            st.write("**å†…å®¹:**")
                            st.text(result['content'][:500] + "..." if len(result['content']) > 500 else result['content'])
                            
                            st.write("**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:**")
                            metadata = result['metadata']
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}")
                                st.write(f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}")
                            with col2:
                                st.write(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}")
                                st.write(f"ç·Šæ€¥åº¦: {metadata.get('urgency_score', 'ä¸æ˜')}")
                else:
                    st.info("é¡ä¼¼ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.warning("æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def search_similar_cases(query: str, max_results: int) -> List[Dict[str, Any]]:
    """é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
    try:
        vector_store = VectorStoreService()
        results = vector_store.search_similar_documents(query, max_results)
        return results
    except Exception as e:
        st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def render_trend_analysis(reports: List[DocumentReport]):
    """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ</div>", unsafe_allow_html=True)
    
    if not reports:
        st.info("åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # æœŸé–“é¸æŠ
    analysis_period = st.selectbox(
        "åˆ†ææœŸé–“ã‚’é¸æŠ:",
        ["éå»7æ—¥é–“", "éå»30æ—¥é–“", "éå»90æ—¥é–“", "å…¨æœŸé–“"]
    )
    
    # æœŸé–“ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_reports = filter_reports_by_period(reports, analysis_period)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆ
    col1, col2 = st.columns(2)
    
    with col1:
        render_issue_trend_chart(filtered_reports)
    
    with col2:
        render_urgency_trend_chart(filtered_reports)
    
    # è©³ç´°çµ±è¨ˆ
    render_trend_statistics(filtered_reports)

def filter_reports_by_period(reports: List[DocumentReport], period: str) -> List[DocumentReport]:
    """æœŸé–“ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    now = datetime.now()
    
    if period == "éå»7æ—¥é–“":
        cutoff = now - timedelta(days=7)
    elif period == "éå»30æ—¥é–“":
        cutoff = now - timedelta(days=30)
    elif period == "éå»90æ—¥é–“":
        cutoff = now - timedelta(days=90)
    else:  # å…¨æœŸé–“
        return reports
    
    return [r for r in reports if r.created_at >= cutoff]

def render_issue_trend_chart(reports: List[DocumentReport]):
    """å•é¡Œç™ºç”Ÿãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"""
    st.write("**å•é¡Œç™ºç”Ÿãƒˆãƒ¬ãƒ³ãƒ‰**")
    
    # æ—¥åˆ¥ã®å•é¡Œæ•°ã‚’é›†è¨ˆ
    daily_issues = {}
    for report in reports:
        date = report.created_at.date()
        if report.analysis_result and report.analysis_result.issues:
            issue_count = len(report.analysis_result.issues)
            daily_issues[date] = daily_issues.get(date, 0) + issue_count
    
    if daily_issues:
        df = pd.DataFrame([
            {"æ—¥ä»˜": date, "å•é¡Œæ•°": count}
            for date, count in sorted(daily_issues.items())
        ])
        
        fig = px.line(
            df, 
            x="æ—¥ä»˜", 
            y="å•é¡Œæ•°",
            title="æ—¥åˆ¥å•é¡Œç™ºç”Ÿæ•°",
            markers=True
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("å•é¡Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def render_urgency_trend_chart(reports: List[DocumentReport]):
    """ç·Šæ€¥åº¦ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"""
    st.write("**ç·Šæ€¥åº¦ãƒˆãƒ¬ãƒ³ãƒ‰**")
    
    # æ—¥åˆ¥ã®å¹³å‡ç·Šæ€¥åº¦ã‚’é›†è¨ˆ
    daily_urgency = {}
    daily_counts = {}
    
    for report in reports:
        date = report.created_at.date()
        if report.analysis_result:
            urgency = getattr(report, 'urgency_score', 0)
            daily_urgency[date] = daily_urgency.get(date, 0) + urgency
            daily_counts[date] = daily_counts.get(date, 0) + 1
    
    if daily_urgency:
        # å¹³å‡ç·Šæ€¥åº¦ã‚’è¨ˆç®—
        avg_urgency = {
            date: daily_urgency[date] / daily_counts[date]
            for date in daily_urgency
        }
        
        df = pd.DataFrame([
            {"æ—¥ä»˜": date, "å¹³å‡ç·Šæ€¥åº¦": urgency}
            for date, urgency in sorted(avg_urgency.items())
        ])
        
        fig = px.line(
            df,
            x="æ—¥ä»˜",
            y="å¹³å‡ç·Šæ€¥åº¦",
            title="æ—¥åˆ¥å¹³å‡ç·Šæ€¥åº¦",
            markers=True,
            range_y=[0, 10]
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ç·Šæ€¥åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def render_trend_statistics(reports: List[DocumentReport]):
    """ãƒˆãƒ¬ãƒ³ãƒ‰çµ±è¨ˆã‚’è¡¨ç¤º"""
    st.write("**çµ±è¨ˆã‚µãƒãƒªãƒ¼**")
    
    if not reports:
        st.info("çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_reports = len(reports)
        st.metric("ç·ãƒ¬ãƒãƒ¼ãƒˆæ•°", total_reports)
    
    with col2:
        avg_urgency = sum(
            getattr(r, 'urgency_score', 0)
            for r in reports
        ) / len(reports)
        st.metric("å¹³å‡ç·Šæ€¥åº¦", f"{avg_urgency:.1f}")
    
    with col3:
        high_urgency_count = len([
            r for r in reports
            if getattr(r, 'urgency_score', 0) >= 7
        ])
        st.metric("é«˜ç·Šæ€¥åº¦æ¡ˆä»¶", high_urgency_count)
    
    with col4:
        emergency_flags = len([
            r for r in reports
            if FlagType.EMERGENCY_STOP in r.flags
        ])
        st.metric("ç·Šæ€¥åœæ­¢æ¡ˆä»¶", emergency_flags)

def render_realtime_analysis():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ</div>", unsafe_allow_html=True)
    
    st.write("**æ–°ã—ã„æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å³åº§ã«åˆ†æ**")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "åˆ†æã—ãŸã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['txt', 'pdf', 'docx'],
        help="ãƒ†ã‚­ã‚¹ãƒˆã€PDFã€Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™"
    )
    
    if uploaded_file is not None:
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­..."):
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            if uploaded_file.type == "text/plain":
                content = str(uploaded_file.read(), "utf-8")
            else:
                content = "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®èª­ã¿è¾¼ã¿ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‡ãƒ¢ç‰ˆï¼‰"
            
            # LLMåˆ†æã‚’å®Ÿè¡Œï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
            try:
                llm_service = get_llm_service()
                analysis_result = llm_service.analyze_document(content)
                anomaly_result = llm_service.detect_anomaly(content)
                
                # çµæœè¡¨ç¤º
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**åˆ†æçµæœ**")
                    st.json(analysis_result)
                
                with col2:
                    st.write("**ç•°å¸¸æ¤œçŸ¥çµæœ**")
                    st.json(anomaly_result)
                
                # æ–‡æ›¸å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.write("**æ–‡æ›¸å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**")
                st.text_area("å†…å®¹", content[:1000] + "..." if len(content) > 1000 else content, height=200)
                
            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ç›£è¦–è¨­å®š
    st.write("**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–è¨­å®š**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        monitor_enabled = st.checkbox("SharePointãƒ•ã‚©ãƒ«ãƒ€ç›£è¦–ã‚’æœ‰åŠ¹åŒ–")
        refresh_interval = st.slider("æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰", 10, 300, 60)
    
    with col2:
        alert_threshold = st.slider("ã‚¢ãƒ©ãƒ¼ãƒˆç·Šæ€¥åº¦ã—ãã„å€¤", 1, 10, 7)
        auto_analysis = st.checkbox("è‡ªå‹•åˆ†æã‚’æœ‰åŠ¹åŒ–")
    
    if monitor_enabled:
        st.info("ğŸ“¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãŒæœ‰åŠ¹ã§ã™ï¼ˆãƒ‡ãƒ¢ç‰ˆã§ã¯å®Ÿéš›ã®ç›£è¦–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ï¼‰")
    
    # æ‰‹å‹•æ›´æ–°ãƒœã‚¿ãƒ³
    if st.button("ğŸ”„ æ‰‹å‹•æ›´æ–°"):
        st.success("æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        st.rerun()