"""
åˆ†æãƒ‘ãƒãƒ«UI
"""
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict, Any
import pandas as pd
from datetime import datetime, timedelta
import logging

from app.models.report import DocumentReport
from app.services.llm_service import get_llm_service
from app.services.vector_store import VectorStoreService
import json
from pathlib import Path

logger = logging.getLogger(__name__)

def load_context_analysis() -> Dict[str, Any]:
    """çµ±åˆåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿"""
    context_file = Path("data/context_analysis/context_analysis.json")
    if context_file.exists():
        try:
            with open(context_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"ğŸ“Š çµ±åˆåˆ†æçµæœèª­ã¿è¾¼ã¿: {len(data)}å·¥ç¨‹ã®åˆ†æçµæœ")
                return data
        except Exception as e:
            st.warning(f"çµ±åˆåˆ†æçµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(f"çµ±åˆåˆ†æçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return {}

def render_analysis_panel(reports: List[DocumentReport], audit_type: str = "å·¥ç¨‹"):
    """åˆ†æãƒ‘ãƒãƒ«ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>AIå¯¾è©±åˆ†æ</div>", unsafe_allow_html=True)
    st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>RAGæŠ€è¡“ã«ã‚ˆã‚‹å ±å‘Šæ›¸æ¤œç´¢ã¨LLMã«ã‚ˆã‚‹è‡ªç„¶è¨€èªã§ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </p>", unsafe_allow_html=True)
    
    # è¡¨ç¤ºè¨­å®šï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸Šéƒ¨ã«å·¦å¯„ã›é…ç½®ï¼‰
    st.markdown("**è¡¨ç¤ºè¨­å®š**")
    col_s1, col_s2, col_spacer = st.columns([2, 2, 6])
    with col_s1:
        use_streaming = st.checkbox("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º", value=True, help="å›ç­”ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º")
    with col_s2:
        show_thinking = st.checkbox("æ€è€ƒéç¨‹è¡¨ç¤º", value=False, help="AIã®æ€è€ƒéç¨‹ã‚’è¡¨ç¤º")
    
    st.divider()
    
    # è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    render_qa_interface(reports, use_streaming, show_thinking, audit_type)

def render_qa_interface(reports: List[DocumentReport], use_streaming: bool = True, show_thinking: bool = False, audit_type: str = "å·¥ç¨‹"):
    """è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¡¨ç¤º"""
    if audit_type == "å ±å‘Šæ›¸":
        st.markdown("<div class='custom-header'>å ±å‘Šæ›¸ã«ã¤ã„ã¦è³ªå•ã™ã‚‹</div>", unsafe_allow_html=True)
        st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>å ±å‘Šæ›¸ã®å†…å®¹ã‚„å“è³ªã«é–¢ã™ã‚‹è³ªå•ã«AIãŒå›ç­”ï¼ˆå ±å‘Šæ›¸ç‰¹åŒ–RAGå‡¦ç†ï¼‰</p>", unsafe_allow_html=True)
    else:
        st.markdown("<div class='custom-header'>å»ºè¨­å·¥ç¨‹ã«ã¤ã„ã¦è³ªå•ã™ã‚‹</div>", unsafe_allow_html=True)
        st.markdown("<p style='color: #666; font-size: 14px; margin-bottom: 16px;'>çµ±åˆåˆ†æçµæœâ†’é–¢é€£å·¥ç¨‹ã®å ±å‘Šæ›¸ã‚’åŠ¹ç‡çš„ã«æ¤œç´¢ã—ã¦AIãŒå›ç­”ï¼ˆåŠ¹ç‡çš„RAGå‡¦ç†ï¼‰</p>", unsafe_allow_html=True)
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ï¼ˆãƒã‚§ãƒƒã‚¯å†…å®¹ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
    st.write("**ã‚µãƒ³ãƒ—ãƒ«è³ªå•:**")
    if audit_type == "å ±å‘Šæ›¸":
        sample_questions = [
            "å ±å‘Šæ›¸ã®è¨˜è¼‰å†…å®¹ã«ä¸å‚™ãŒã‚ã‚‹ã‚‚ã®ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "å¿…é ˆé …ç›®ãŒä¸è¶³ã—ã¦ã„ã‚‹å ±å‘Šæ›¸ã‚’æ•™ãˆã¦ãã ã•ã„",
            "é…å»¶ç†ç”±ã®åˆ†é¡ãŒå›°é›£ãªå ±å‘Šæ›¸ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "LLMã®åˆ†æä¿¡é ¼åº¦ãŒä½ã„å ±å‘Šæ›¸ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿ",
            "å ±å‘Šæ›¸ã®å“è³ªã«å•é¡ŒãŒã‚ã‚‹ã‚‚ã®ã‚’ç‰¹å®šã—ã¦ãã ã•ã„"
        ]
    else:
        sample_questions = [
            "ç¾åœ¨é€²è¡Œä¸­ã®ãƒˆãƒ©ãƒ–ãƒ«å·¥ç¨‹ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "æœ€ã‚‚ç·Šæ€¥åº¦ã®é«˜ã„å·¥ç¨‹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "ä½æ°‘åå¯¾ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ç¾å ´ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "å·¥æœŸé…å»¶ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹å·¥ç¨‹ã‚’æ•™ãˆã¦ãã ã•ã„",
            "è¨­å‚™ä¸å…·åˆãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ç¾å ´ã¯ã©ã“ã§ã™ã‹ï¼Ÿ"
        ]
    
    selected_question = st.selectbox(
        "ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠï¼ˆã¾ãŸã¯ä¸‹ã«ç‹¬è‡ªã®è³ªå•ã‚’å…¥åŠ›ï¼‰",
        ["è³ªå•ã‚’é¸æŠ..."] + sample_questions
    )
    
    # è³ªå•å…¥åŠ›
    if selected_question != "è³ªå•ã‚’é¸æŠ...":
        question = st.text_input("è³ªå•å†…å®¹:", value=selected_question)
    else:
        question = st.text_input("è³ªå•å†…å®¹:")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªAIè³ªå•ãƒœã‚¿ãƒ³ï¼ˆè¨­å®šãªã—ï¼‰
    ask_button = st.button("AIã«è³ªå•ã™ã‚‹", type="primary", use_container_width=True)
    
    if ask_button:
        if question:
            st.write("**RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹AIå›ç­”:**")
            
            # RAGã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œå¯è¦–åŒ–
            with st.spinner("ğŸ” é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ä¸­..."):
                # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®å®Ÿè¡Œã¨çµæœè¡¨ç¤º
                vector_store = VectorStoreService()
                search_results = vector_store.search_similar_documents(
                    query=question, 
                    n_results=8
                )
                
                # æ¤œç´¢çµæœã®å¯è¦–åŒ–
                if search_results:
                    # æ­£è¦åŒ–ã•ã‚ŒãŸé¡ä¼¼åº¦ã§é–¢é€£æ–‡æ›¸ã‚’åˆ¤å®š
                    relevant_docs = []
                    for r in search_results:
                        distance = r.get('distance', 0.0)
                        similarity_score = 1.0 / (1.0 + distance / 100.0)
                        if similarity_score > 0.1:
                            relevant_docs.append((r, similarity_score))
                    
                    # é–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ãŒãªã„å ´åˆã¯ä¸Šä½3ä»¶ã‚’ä½¿ç”¨
                    if not relevant_docs:
                        for r in search_results[:3]:
                            distance = r.get('distance', 0.0)
                            similarity_score = 1.0 / (1.0 + distance / 100.0)
                            relevant_docs.append((r, similarity_score))
                    
                    if relevant_docs:
                        st.success(f"âœ… {len(relevant_docs)}ä»¶ã®é–¢é€£æ–‡æ›¸ã‚’ç™ºè¦‹")
                        
                        # æ¤œç´¢çµæœã®è©³ç´°è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if show_thinking:
                            with st.expander("ğŸ” æ¤œç´¢ã•ã‚ŒãŸé–¢é€£æ–‡æ›¸"):
                                for i, (result, similarity) in enumerate(relevant_docs[:3]):
                                    metadata = result.get('metadata', {})
                                    st.write(f"**{i+1}. {metadata.get('file_name', 'ä¸æ˜')}** (é¡ä¼¼åº¦: {similarity:.3f})")
                                    st.write(f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}")
                                    st.write(f"å†…å®¹æŠœç²‹: {result.get('content', '')[:150]}...")
                                    st.divider()
                    else:
                        st.success("âœ… é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã—ã¾ã—ãŸï¼ˆä¸Šä½çµæœã‚’ä½¿ç”¨ï¼‰")
                else:
                    st.warning("âš ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨")
            
            # æ€è€ƒéç¨‹è¡¨ç¤º
            if show_thinking:
                with st.spinner("ğŸ§  AIãŒæ–‡æ›¸ã‚’åˆ†æä¸­..."):
                    import time
                    time.sleep(1)  # æ€è€ƒæ¼”å‡º
                st.success("ğŸ’¡ å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™")
            
            # çµ±ä¸€ã•ã‚ŒãŸå›ç­”è¡¨ç¤ºã‚³ãƒ³ãƒ†ãƒŠï¼ˆå…ƒã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
            response_placeholder = st.empty()
            
            if use_streaming:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºï¼ˆå…ƒã®infoé¢¨ã‚¹ã‚¿ã‚¤ãƒ«å†…ã§ï¼‰
                full_response = ""
                chunk_count = 0
                
                for chunk in process_qa_question_stream(question, reports):
                    full_response += chunk
                    chunk_count += 1
                    
                    # 3æ–‡å­—ã”ã¨ã«æ›´æ–°ï¼ˆå…ƒã®infoé¢¨ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰
                    if chunk_count % 3 == 0:
                        with response_placeholder.container():
                            st.info(f"{full_response}â–Œ")  # ã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ã‚½ãƒ«ä»˜ã
                
                # æœ€çµ‚è¡¨ç¤ºï¼ˆã‚«ãƒ¼ã‚½ãƒ«å‰Šé™¤ï¼‰
                with response_placeholder.container():
                    st.info(full_response)
                
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
                
            else:
                # å¾“æ¥ã®ä¸€æ‹¬è¡¨ç¤ºï¼ˆå…ƒã®ã‚¹ã‚¿ã‚¤ãƒ«ç¶­æŒï¼‰
                if show_thinking:
                    with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = process_qa_question(question, reports, audit_type)
                else:
                    with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = process_qa_question(question, reports, audit_type)
                
                # å…ƒã®ã‚·ãƒ³ãƒ—ãƒ«ãªinfoè¡¨ç¤º
                with response_placeholder.container():
                    st.info(answer)
                
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def process_qa_question(question: str, reports: List[DocumentReport], audit_type: str = "å·¥ç¨‹") -> str:
    """åŠ¹ç‡çš„ãªRAGå‡¦ç†ã«ã‚ˆã‚‹è³ªå•å¿œç­”ï¼ˆãƒã‚§ãƒƒã‚¯å†…å®¹ã«å¿œã˜ã¦æ¤œç´¢æ–¹æ³•ã‚’å¤‰æ›´ï¼‰"""
    try:
        vector_store = VectorStoreService()
        
        if audit_type == "å ±å‘Šæ›¸":
            # å ±å‘Šæ›¸ãƒã‚§ãƒƒã‚¯ï¼šå ±å‘Šæ›¸è¦ç´„ã®å‡ºåŠ›çµæœã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            return _process_report_audit_question(question, vector_store)
        else:
            # å·¥ç¨‹ãƒã‚§ãƒƒã‚¯ï¼šçµ±åˆåˆ†æçµæœã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            return _process_project_audit_question(question, reports, vector_store)
        
    except Exception as e:
        return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def _process_report_audit_question(question: str, vector_store: VectorStoreService) -> str:
    """å ±å‘Šæ›¸ãƒã‚§ãƒƒã‚¯ç”¨ã®è³ªå•å‡¦ç†ï¼šå ±å‘Šæ›¸è¦ç´„ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã—ã¦ä¸Šä½5ä»¶ã‚’å–å¾—"""
    try:
        # å ±å‘Šæ›¸è¦ç´„ã®å‡ºåŠ›çµæœã‚’æ¤œç´¢ï¼ˆçµ±åˆåˆ†æçµæœã‚’é™¤å¤–ï¼‰
        search_results = vector_store.search_similar_documents(
            query=question,
            n_results=10  # å¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        )
        
        # çµ±åˆåˆ†æçµæœã‚’é™¤å¤–ã—ã€å ±å‘Šæ›¸è¦ç´„ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        filtered_results = [
            result for result in search_results 
            if result.get('metadata', {}).get('type') != 'context_analysis'
        ]
        
        # ä¸Šä½5ä»¶ã‚’å–å¾—ï¼ˆé¡ä¼¼åº¦é–¾å€¤ã¯ä½¿ã‚ãªã„ï¼‰
        top_5_results = filtered_results[:5]
        
        context_parts = []
        for i, result in enumerate(top_5_results):
            distance = result.get('distance', 0.0)
            similarity_score = 1.0 / (1.0 + distance / 100.0)
            metadata = result.get('metadata', {})
            content = result.get('content', '')
            
            context_parts.append(
                f"=== å ±å‘Šæ›¸è¦ç´„{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}) ===\\n"
                f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {metadata.get('status_flag', 'ä¸æ˜')}\\n"
                f"è¦ç´„å†…å®¹: {content[:400]}...\\n"
            )
        
        if not context_parts:
            return "é–¢é€£ã™ã‚‹å ±å‘Šæ›¸è¦ç´„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        # LLMã«è³ªå•
        context = "\\n".join(context_parts)
        llm_service = get_llm_service()
        answer = llm_service.answer_question(question, context)
        
        logger.info(f"ğŸ“‹ å ±å‘Šæ›¸ãƒã‚§ãƒƒã‚¯è³ªå•å‡¦ç†: {len(top_5_results)}ä»¶ã®å ±å‘Šæ›¸è¦ç´„ã‚’ä½¿ç”¨")
        return answer
        
    except Exception as e:
        return f"å ±å‘Šæ›¸ãƒã‚§ãƒƒã‚¯ã®è³ªå•å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def _process_project_audit_question(question: str, reports: List[DocumentReport], vector_store: VectorStoreService) -> str:
    """å·¥ç¨‹ãƒã‚§ãƒƒã‚¯ç”¨ã®è³ªå•å‡¦ç†ï¼šçµ±åˆåˆ†æçµæœã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã—ã¦ä¸Šä½5ä»¶ã‚’å–å¾—"""
    try:
        # ğŸ” Step 1: çµ±åˆåˆ†æçµæœã‹ã‚‰é–¢é€£å·¥ç¨‹ã‚’æ¤œç´¢ï¼ˆä¸Šä½5ä»¶ï¼‰
        context_results = vector_store.search_similar_documents(
            query=question,
            n_results=5,
            filter_metadata={'type': 'context_analysis'}  # çµ±åˆåˆ†æçµæœã®ã¿æ¤œç´¢
        )
        
        if not context_results:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢
            return _fallback_search(question, reports, vector_store)
        
        # ğŸ¯ Step 2: é–¢é€£å·¥ç¨‹IDã‚’ç‰¹å®šï¼ˆä¸Šä½5ä»¶ã™ã¹ã¦ä½¿ç”¨ï¼‰
        related_project_ids = []
        context_parts = []
        
        # çµ±åˆåˆ†æçµæœã®ã‚µãƒãƒªã‚’è¿½åŠ 
        context_analysis = load_context_analysis()
        if context_analysis:
            context_parts.append("=== å…¨å·¥ç¨‹çµ±åˆåˆ†æã‚µãƒãƒª ===")
            for project_id, analysis in list(context_analysis.items())[:3]:  # ä¸Šä½3å·¥ç¨‹ã®ã‚µãƒãƒª
                context_parts.append(
                    f"å·¥ç¨‹ID: {project_id}\\n"
                    f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {analysis.get('overall_status', 'ä¸æ˜')}\\n"
                    f"ç·åˆãƒªã‚¹ã‚¯: {analysis.get('overall_risk', 'ä¸æ˜')}\\n"
                    f"ç¾åœ¨å·¥ç¨‹: {analysis.get('current_phase', 'ä¸æ˜')}\\n"
                    f"é€²æ—å‚¾å‘: {analysis.get('progress_trend', 'ä¸æ˜')}\\n"
                    f"åˆ†æã‚µãƒãƒª: {analysis.get('analysis_summary', '')}\\n"
                )
            context_parts.append("")
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‹ã‚‰é–¢é€£å·¥ç¨‹ã‚’ç‰¹å®š
        for i, result in enumerate(context_results):
            distance = result.get('distance', 0.0)
            similarity_score = 1.0 / (1.0 + distance / 100.0)
            metadata = result.get('metadata', {})
            project_id = metadata.get('project_id')
            
            if project_id and project_id not in related_project_ids:
                related_project_ids.append(project_id)
                
                # çµ±åˆåˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
                context_parts.append(
                    f"=== é–¢é€£å·¥ç¨‹çµ±åˆåˆ†æçµæœ{i+1} ({project_id}) ===\\n"
                    f"é¡ä¼¼åº¦: {similarity_score:.3f}\\n"
                    f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {metadata.get('overall_status', 'ä¸æ˜')}\\n"
                    f"ç·åˆãƒªã‚¹ã‚¯: {metadata.get('overall_risk', 'ä¸æ˜')}\\n"
                    f"ç¾åœ¨å·¥ç¨‹: {metadata.get('current_phase', 'ä¸æ˜')}\\n"
                    f"é€²æ—å‚¾å‘: {metadata.get('progress_trend', 'ä¸æ˜')}\\n"
                    f"å†…å®¹: {result.get('content', '')[:300]}...\\n"
                )
        
        # ğŸ“„ Step 3: é–¢é€£å·¥ç¨‹ã®å ±å‘Šæ›¸è¦ç´„ã‚’ã™ã¹ã¦å–å¾—
        if related_project_ids:
            reports_by_project = _load_specific_reports_by_project_ids(related_project_ids)
            
            for project_id in related_project_ids:
                if project_id in reports_by_project:
                    project_reports = reports_by_project[project_id]
                    context_parts.append(f"\\n=== å·¥ç¨‹ {project_id} ã®é–¢é€£å ±å‘Šæ›¸è¦ç´„ ===")
                    
                    for i, report in enumerate(project_reports):  # å·¥ç¨‹ã®å…¨å ±å‘Šæ›¸
                        context_parts.append(
                            f"å ±å‘Šæ›¸{i+1}: {report.get('file_name', 'ä¸æ˜')}\\n"
                            f"è¦ç´„: {report.get('analysis_result', {}).get('summary', '')}\\n"
                            f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {report.get('risk_level', 'ä¸æ˜')}\\n"
                            f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {report.get('status_flag', 'ä¸æ˜')}\\n"
                            f"å•é¡Œ: {', '.join(report.get('analysis_result', {}).get('issues', []))}\\n"
                        )
        
        # ğŸ¤– Step 4: LLMã«è³ªå•
        context = "\\n".join(context_parts)
        llm_service = get_llm_service()
        answer = llm_service.answer_question(question, context)
        
        logger.info(f"ğŸ—ï¸ å·¥ç¨‹ãƒã‚§ãƒƒã‚¯è³ªå•å‡¦ç†: {len(related_project_ids)}å·¥ç¨‹ã€{sum(len(reports_by_project.get(pid, [])) for pid in related_project_ids)}ä»¶ã®å ±å‘Šæ›¸è¦ç´„ã‚’ä½¿ç”¨")
        return answer
        
    except Exception as e:
        return f"å·¥ç¨‹ãƒã‚§ãƒƒã‚¯ã®è³ªå•å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def _load_specific_reports_by_project_ids(project_ids: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """æŒ‡å®šã•ã‚ŒãŸå·¥ç¨‹IDã®å ±å‘Šæ›¸ã®ã¿ã‚’èª­ã¿è¾¼ã¿"""
    reports_by_project = {}
    processed_dir = Path("data/processed_reports")
    
    if not processed_dir.exists():
        return {}
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    index_file = processed_dir / "index.json"
    if index_file.exists():
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            # æˆåŠŸã—ãŸå‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
            successful_files = {k: v for k, v in index_data.get("processed_files", {}).items() 
                              if v.get("status") == "success"}
            
            for file_key, file_info in successful_files.items():
                json_file_path = file_info.get("result_file")
                if json_file_path:
                    json_file = Path(json_file_path)
                    if json_file.exists():
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                report_data = json.load(f)
                            
                            project_id = report_data.get('project_id')
                            # æŒ‡å®šã•ã‚ŒãŸå·¥ç¨‹IDã®å ±å‘Šæ›¸ã®ã¿ã‚’èª­ã¿è¾¼ã¿
                            if project_id and project_id in project_ids:
                                if project_id not in reports_by_project:
                                    reports_by_project[project_id] = []
                                reports_by_project[project_id].append(report_data)
                                
                        except Exception as e:
                            logger.warning(f"å ±å‘Šæ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {json_file.name} - {e}")
            
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    logger.info(f"ğŸ“Š æŒ‡å®šå·¥ç¨‹ã®å ±å‘Šæ›¸èª­ã¿è¾¼ã¿: {len(reports_by_project)}å·¥ç¨‹ã€{sum(len(reports) for reports in reports_by_project.values())}ä»¶ã®å ±å‘Šæ›¸")
    return reports_by_project

def _fallback_search(question: str, reports: List[DocumentReport], vector_store: VectorStoreService) -> str:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢"""
    try:
        # é€šå¸¸ã®å ±å‘Šæ›¸æ¤œç´¢ï¼ˆçµ±åˆåˆ†æçµæœä»¥å¤–ï¼‰
        search_results = vector_store.search_similar_documents(
            query=question,
            n_results=8
        )
        
        # çµ±åˆåˆ†æçµæœã‚’é™¤å¤–
        filtered_results = [
            result for result in search_results 
            if result.get('metadata', {}).get('type') != 'context_analysis'
        ]
        
        # ä¸Šä½5ä»¶ã‚’å–å¾—ï¼ˆé¡ä¼¼åº¦é–¾å€¤ã¯ä½¿ã‚ãªã„ï¼‰
        top_5_results = filtered_results[:5]
        
        context_parts = []
        for i, result in enumerate(top_5_results):
            distance = result.get('distance', 0.0)
            similarity_score = 1.0 / (1.0 + distance / 100.0)
            metadata = result.get('metadata', {})
            content = result.get('content', '')
            
            context_parts.append(
                f"é–¢é€£æ–‡æ›¸{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}):\\n"
                f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                f"å†…å®¹: {content[:300]}...\\n"
            )
        
        # çµ±åˆåˆ†æçµæœã‚‚è¿½åŠ ï¼ˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        context_analysis = load_context_analysis()
        if context_analysis:
            context_parts.append("\\n=== æ¡ˆä»¶çµ±åˆåˆ†æçµæœ ===")
            for project_id, analysis in list(context_analysis.items())[:3]:  # ä¸Šä½3ä»¶
                context_parts.append(
                    f"æ¡ˆä»¶ID: {project_id}\\n"
                    f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {analysis.get('overall_status', 'ä¸æ˜')}\\n"
                    f"ç·åˆãƒªã‚¹ã‚¯: {analysis.get('overall_risk', 'ä¸æ˜')}\\n"
                    f"åˆ†æã‚µãƒãƒª: {analysis.get('analysis_summary', '')}\\n"
                )
        
        if not context_parts:
            return "é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        context = "\\n".join(context_parts)
        llm_service = get_llm_service()
        answer = llm_service.answer_question(question, context)
        
        logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢: {len(top_5_results)}ä»¶ã®æ–‡æ›¸ã‚’ä½¿ç”¨")
        return answer
        
    except Exception as e:
        return f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def process_qa_question_stream(question: str, reports: List[DocumentReport]):
    """è³ªå•å¿œç­”ã‚’å‡¦ç†ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œãƒ»RAGã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
    try:
        # ğŸ” RAGã‚·ã‚¹ãƒ†ãƒ : è³ªå•å†…å®¹ã«åŸºã¥ã„ã¦é–¢é€£æ–‡æ›¸ã‚’å‹•çš„æ¤œç´¢
        vector_store = VectorStoreService()
        search_results = vector_store.search_similar_documents(
            query=question, 
            n_results=8  # ã‚ˆã‚Šå¤šãã®é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        )
        
        # æ¤œç´¢çµæœã‹ã‚‰é«˜å“è³ªãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context_parts = []
        
        # ä¸Šä½5ä»¶ã‚’å–å¾—ï¼ˆé¡ä¼¼åº¦é–¾å€¤ã¯ä½¿ã‚ãªã„ï¼‰
        top_5_results = search_results[:5] if search_results else []
        
        for i, result in enumerate(top_5_results):
            distance = result.get('distance', 0.0)
            similarity_score = 1.0 / (1.0 + distance / 100.0)
            metadata = result.get('metadata', {})
            content = result.get('content', '')
            
            context_parts.append(
                f"é–¢é€£æ–‡æ›¸{i+1} (é¡ä¼¼åº¦: {similarity_score:.3f}):\\n"
                f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}\\n"
                f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}\\n"
                f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}\\n"
                f"å†…å®¹: {content[:300]}...\\n"
            )
        
        # ğŸ†• çµ±åˆåˆ†æçµæœã‚’è¿½åŠ 
        context_analysis = load_context_analysis()
        if context_analysis:
            context_parts.append("\\n=== æ¡ˆä»¶çµ±åˆåˆ†æçµæœ ===")
            for project_id, analysis in context_analysis.items():
                context_parts.append(
                    f"æ¡ˆä»¶ID: {project_id}\\n"
                    f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {analysis.get('overall_status', 'ä¸æ˜')}\\n"
                    f"ç·åˆãƒªã‚¹ã‚¯: {analysis.get('overall_risk', 'ä¸æ˜')}\\n"
                    f"ç¾åœ¨å·¥ç¨‹: {analysis.get('current_phase', 'ä¸æ˜')}\\n"
                    f"é€²æ—å‚¾å‘: {analysis.get('progress_trend', 'ä¸æ˜')}\\n"
                    f"å•é¡Œç¶™ç¶šæ€§: {analysis.get('issue_continuity', 'ä¸æ˜')}\\n"
                    f"åˆ†æã‚µãƒãƒª: {analysis.get('analysis_summary', '')}\\n"
                )
                
                # é…å»¶ç†ç”±ç®¡ç†æƒ…å ±
                delay_reasons = analysis.get('delay_reasons_management', [])
                if delay_reasons:
                    context_parts.append(f"ç¾åœ¨ã®é…å»¶ç†ç”±ãƒ»å•é¡Œ:")
                    for reason in delay_reasons[:3]:  # ä¸Šä½3ä»¶
                        context_parts.append(
                            f"  - {reason.get('delay_category', '')}/{reason.get('delay_subcategory', '')}: "
                            f"{reason.get('description', '')} (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {reason.get('status', '')})"
                        )
                
                # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                actions = analysis.get('recommended_actions', [])
                if actions:
                    context_parts.append(f"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {', '.join(actions[:3])}")
                
                context_parts.append("---")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã§çµæœãŒå°‘ãªã„å ´åˆã¯æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚‚è¿½åŠ 
        if len([p for p in context_parts if not p.startswith("=== æ¡ˆä»¶çµ±åˆåˆ†æçµæœ")]) < 3:
            for i, report in enumerate(reports[:5]):
                if report.analysis_result:
                    context_parts.append(
                        f"æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆ{i+1}: {report.file_name}\\n"
                        f"è¦ç´„: {report.analysis_result.summary}\\n"
                        f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {getattr(report, 'risk_level', 'ä¸æ˜')}\\n"
                        f"å•é¡Œ: {', '.join(report.analysis_result.issues)}\\n"
                    )
        
        context = "\\n".join(context_parts)
        
        # LLMã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è³ªå•ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
        llm_service = get_llm_service()
        yield from llm_service.answer_question_stream(question, context)
        
    except Exception as e:
        yield f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def render_similarity_search():
    """é¡ä¼¼ã‚±ãƒ¼ã‚¹æ¤œç´¢ã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>é¡ä¼¼ã‚±ãƒ¼ã‚¹æ¤œç´¢</div>", unsafe_allow_html=True)
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›
    search_query = st.text_input(
        "æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: ä½æ°‘åå¯¾ã«ã‚ˆã‚‹å·¥äº‹åœæ­¢"
    )
    
    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    col1, col2 = st.columns(2)
    with col1:
        max_results = st.slider("æœ€å¤§è¡¨ç¤ºä»¶æ•°", 1, 20, 5)
    with col2:
        similarity_threshold = st.slider("é¡ä¼¼åº¦é–¾å€¤", 0.0, 1.0, 0.5)
    
    if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ"):
        if search_query:
            with st.spinner("é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­..."):
                results = search_similar_cases(search_query, max_results)
                
                if results:
                    st.write(f"**{len(results)}ä»¶ã®é¡ä¼¼ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:**")
                    
                    for i, result in enumerate(results, 1):
                        distance = result.get('distance', 0.0)
                        similarity_score = 1.0 / (1.0 + distance / 100.0)
                        with st.expander(f"{i}. {result['metadata'].get('file_name', 'ä¸æ˜')} (é¡ä¼¼åº¦: {similarity_score:.3f})"):
                            st.write("**å†…å®¹:**")
                            st.text(result['content'][:500] + "..." if len(result['content']) > 500 else result['content'])
                            
                            st.write("**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:**")
                            metadata = result['metadata']
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.get('file_name', 'ä¸æ˜')}")
                                st.write(f"ãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥: {metadata.get('report_type', 'ä¸æ˜')}")
                            with col2:
                                st.write(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metadata.get('risk_level', 'ä¸æ˜')}")
                                st.write(f"ç·Šæ€¥åº¦: {metadata.get('urgency_score', 'ä¸æ˜')}")
                else:
                    st.info("é¡ä¼¼ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.warning("æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def search_similar_cases(query: str, max_results: int) -> List[Dict[str, Any]]:
    """é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
    try:
        vector_store = VectorStoreService()
        results = vector_store.search_similar_documents(query, max_results)
        return results
    except Exception as e:
        st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def render_trend_analysis(reports: List[DocumentReport]):
    """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ</div>", unsafe_allow_html=True)
    
    if not reports:
        st.info("åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # æœŸé–“é¸æŠ
    analysis_period = st.selectbox(
        "åˆ†ææœŸé–“ã‚’é¸æŠ:",
        ["éå»7æ—¥é–“", "éå»30æ—¥é–“", "éå»90æ—¥é–“", "å…¨æœŸé–“"]
    )
    
    # æœŸé–“ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_reports = filter_reports_by_period(reports, analysis_period)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆ
    col1, col2 = st.columns(2)
    
    with col1:
        render_issue_trend_chart(filtered_reports)
    
    with col2:
        render_urgency_trend_chart(filtered_reports)
    
    # è©³ç´°çµ±è¨ˆ
    render_trend_statistics(filtered_reports)

def filter_reports_by_period(reports: List[DocumentReport], period: str) -> List[DocumentReport]:
    """æœŸé–“ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    now = datetime.now()
    
    if period == "éå»7æ—¥é–“":
        cutoff = now - timedelta(days=7)
    elif period == "éå»30æ—¥é–“":
        cutoff = now - timedelta(days=30)
    elif period == "éå»90æ—¥é–“":
        cutoff = now - timedelta(days=90)
    else:  # å…¨æœŸé–“
        return reports
    
    return [r for r in reports if r.created_at >= cutoff]

def render_issue_trend_chart(reports: List[DocumentReport]):
    """å•é¡Œç™ºç”Ÿãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"""
    st.write("**å•é¡Œç™ºç”Ÿãƒˆãƒ¬ãƒ³ãƒ‰**")
    
    # æ—¥åˆ¥ã®å•é¡Œæ•°ã‚’é›†è¨ˆ
    daily_issues = {}
    for report in reports:
        date = report.created_at.date()
        if report.analysis_result and report.analysis_result.issues:
            issue_count = len(report.analysis_result.issues)
            daily_issues[date] = daily_issues.get(date, 0) + issue_count
    
    if daily_issues:
        df = pd.DataFrame([
            {"æ—¥ä»˜": date, "å•é¡Œæ•°": count}
            for date, count in sorted(daily_issues.items())
        ])
        
        fig = px.line(
            df, 
            x="æ—¥ä»˜", 
            y="å•é¡Œæ•°",
            title="æ—¥åˆ¥å•é¡Œç™ºç”Ÿæ•°",
            markers=True
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("å•é¡Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def render_urgency_trend_chart(reports: List[DocumentReport]):
    """ç·Šæ€¥åº¦ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"""
    st.write("**ç·Šæ€¥åº¦ãƒˆãƒ¬ãƒ³ãƒ‰**")
    
    # æ—¥åˆ¥ã®å¹³å‡ç·Šæ€¥åº¦ã‚’é›†è¨ˆ
    daily_urgency = {}
    daily_counts = {}
    
    for report in reports:
        date = report.created_at.date()
        if report.analysis_result:
            urgency = getattr(report, 'urgency_score', 0)
            daily_urgency[date] = daily_urgency.get(date, 0) + urgency
            daily_counts[date] = daily_counts.get(date, 0) + 1
    
    if daily_urgency:
        # å¹³å‡ç·Šæ€¥åº¦ã‚’è¨ˆç®—
        avg_urgency = {
            date: daily_urgency[date] / daily_counts[date]
            for date in daily_urgency
        }
        
        df = pd.DataFrame([
            {"æ—¥ä»˜": date, "å¹³å‡ç·Šæ€¥åº¦": urgency}
            for date, urgency in sorted(avg_urgency.items())
        ])
        
        fig = px.line(
            df,
            x="æ—¥ä»˜",
            y="å¹³å‡ç·Šæ€¥åº¦",
            title="æ—¥åˆ¥å¹³å‡ç·Šæ€¥åº¦",
            markers=True,
            range_y=[0, 10]
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ç·Šæ€¥åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def render_trend_statistics(reports: List[DocumentReport]):
    """ãƒˆãƒ¬ãƒ³ãƒ‰çµ±è¨ˆã‚’è¡¨ç¤º"""
    st.write("**çµ±è¨ˆã‚µãƒãƒªãƒ¼**")
    
    if not reports:
        st.info("çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_reports = len(reports)
        st.metric("ç·ãƒ¬ãƒãƒ¼ãƒˆæ•°", total_reports)
    
    with col2:
        avg_urgency = sum(
            getattr(r, 'urgency_score', 0)
            for r in reports
        ) / len(reports)
        st.metric("å¹³å‡ç·Šæ€¥åº¦", f"{avg_urgency:.1f}")
    
    with col3:
        high_urgency_count = len([
            r for r in reports
            if getattr(r, 'urgency_score', 0) >= 7
        ])
        st.metric("é«˜ç·Šæ€¥åº¦æ¡ˆä»¶", high_urgency_count)
    
    with col4:
        emergency_flags = len([
            r for r in reports
            if FlagType.EMERGENCY_STOP in r.flags
        ])
        st.metric("ç·Šæ€¥åœæ­¢æ¡ˆä»¶", emergency_flags)

def render_realtime_analysis():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("<div class='custom-header'>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ</div>", unsafe_allow_html=True)
    
    st.write("**æ–°ã—ã„æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å³åº§ã«åˆ†æ**")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "åˆ†æã—ãŸã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['txt', 'pdf', 'docx'],
        help="ãƒ†ã‚­ã‚¹ãƒˆã€PDFã€Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™"
    )
    
    if uploaded_file is not None:
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­..."):
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            if uploaded_file.type == "text/plain":
                content = str(uploaded_file.read(), "utf-8")
            else:
                content = "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®èª­ã¿è¾¼ã¿ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‡ãƒ¢ç‰ˆï¼‰"
            
            # LLMåˆ†æã‚’å®Ÿè¡Œï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
            try:
                llm_service = get_llm_service()
                analysis_result = llm_service.analyze_document(content)
                anomaly_result = llm_service.detect_anomaly(content)
                
                # çµæœè¡¨ç¤º
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**åˆ†æçµæœ**")
                    st.json(analysis_result)
                
                with col2:
                    st.write("**ç•°å¸¸æ¤œçŸ¥çµæœ**")
                    st.json(anomaly_result)
                
                # æ–‡æ›¸å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.write("**æ–‡æ›¸å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**")
                st.text_area("å†…å®¹", content[:1000] + "..." if len(content) > 1000 else content, height=200)
                
            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ç›£è¦–è¨­å®š
    st.write("**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–è¨­å®š**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        monitor_enabled = st.checkbox("SharePointãƒ•ã‚©ãƒ«ãƒ€ç›£è¦–ã‚’æœ‰åŠ¹åŒ–")
        refresh_interval = st.slider("æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰", 10, 300, 60)
    
    with col2:
        alert_threshold = st.slider("ã‚¢ãƒ©ãƒ¼ãƒˆç·Šæ€¥åº¦ã—ãã„å€¤", 1, 10, 7)
        auto_analysis = st.checkbox("è‡ªå‹•åˆ†æã‚’æœ‰åŠ¹åŒ–")
    
    if monitor_enabled:
        st.info("ğŸ“¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãŒæœ‰åŠ¹ã§ã™ï¼ˆãƒ‡ãƒ¢ç‰ˆã§ã¯å®Ÿéš›ã®ç›£è¦–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ï¼‰")
    
    # æ‰‹å‹•æ›´æ–°ãƒœã‚¿ãƒ³
    if st.button("ğŸ”„ æ‰‹å‹•æ›´æ–°"):
        st.success("æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        st.rerun()