#!/usr/bin/env python3
"""
äº‹å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: ãƒ¬ãƒãƒ¼ãƒˆæŠ•å…¥æ™‚ã®å‡¦ç†ã‚’å®Ÿè¡Œ
ä½¿ç”¨æ–¹æ³•: python scripts/preprocess_documents.py [--provider ollama]
"""
import sys
import os
import json
import pickle
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.document_processor import DocumentProcessor
from app.services.vector_store import VectorStoreService
from app.config.settings import SHAREPOINT_DOCS_DIR

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingService:
    """äº‹å‰å‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, llm_provider: str = "ollama"):
        self.document_processor = DocumentProcessor(llm_provider=llm_provider, create_vector_store=True)
        self.vector_store = VectorStoreService(create_mode=True)
        self.results_dir = project_root / "data" / "processed_reports"
        self.index_file = self.results_dir / "index.json"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def _load_index(self) -> Dict[str, Any]:
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        if self.index_file.exists():
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"processed_files": {}, "last_updated": datetime.now().isoformat(), "version": "1.0"}
    
    def _save_index(self, index: Dict[str, Any]):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜"""
        index["last_updated"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰"""
        import hashlib
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _is_file_processed(self, file_path: Path, index: Dict[str, Any]) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        file_key = str(file_path.relative_to(project_root))
        if file_key not in index["processed_files"]:
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’ãƒãƒƒã‚·ãƒ¥ã§æ¤œå‡º
        current_hash = self._get_file_hash(file_path)
        stored_info = index["processed_files"][file_key]
        return stored_info.get("file_hash") == current_hash
    
    def process_single_file(self, file_path: Path, force: bool = False) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰å‡¦ç†"""
        index = self._load_index()
        file_key = str(file_path.relative_to(project_root))
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
        if not force and self._is_file_processed(file_path, index):
            logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰: {file_path.name}")
            return {"status": "skipped", "reason": "already_processed"}
        
        try:
            logger.info(f"ğŸ”„ å‡¦ç†ä¸­: {file_path.name}")
            start_time = datetime.now()
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
            report = self.document_processor.process_single_document(file_path)
            
            if report:
                # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                self.vector_store.add_document(
                    content=report.content,
                    metadata={
                        "file_path": str(file_path),
                        "file_name": file_path.name,
                        "report_type": report.report_type.value if report.report_type else "unknown",
                        "processed_at": datetime.now().isoformat(),
                        "flags": ",".join([flag.value for flag in report.flags]) if report.flags else "",
                        "risk_level": report.risk_level.value if report.risk_level else "ä½",
                        "has_anomaly": report.anomaly_detection.has_anomaly if report.anomaly_detection else False
                    }
                )
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦çµæœä¿å­˜
                result_data = self._serialize_report(report)
                result_file = self.results_dir / f"{file_path.stem}.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                # ğŸš€ ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆé«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ï¼‰
                cache_file = self.results_dir / f"{file_path.stem}.cache"
                try:
                    with open(cache_file, 'wb') as f:
                        pickle.dump(report, f)
                    logger.debug(f"ğŸ’¾ Binary cache saved: {cache_file.name}")
                except Exception as e:
                    logger.warning(f"Failed to save binary cache {cache_file}: {e}")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
                processing_time = (datetime.now() - start_time).total_seconds()
                index["processed_files"][file_key] = {
                    "file_hash": self._get_file_hash(file_path),
                    "processed_at": datetime.now().isoformat(),
                    "processing_time": processing_time,
                    "result_file": str(result_file.relative_to(project_root)),
                    "cache_file": str(cache_file.relative_to(project_root)),
                    "status": "success"
                }
                self._save_index(index)
                
                logger.info(f"âœ… å‡¦ç†å®Œäº†: {file_path.name} ({processing_time:.1f}ç§’)")
                return {"status": "success", "processing_time": processing_time}
            else:
                raise Exception("Document processing failed")
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨˜éŒ²
            index["processed_files"][file_key] = {
                "file_hash": self._get_file_hash(file_path),
                "processed_at": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }
            self._save_index(index)
            
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {file_path.name} - {str(e)}")
            return {"status": "error", "error": str(e)}
        
    def process_all_documents(self, force: bool = False) -> Dict[str, Any]:
        """å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®äº‹å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        logger.info("äº‹å‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        start_time = datetime.now()
        
        # SharePointãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        doc_files = self._get_all_document_files()
        logger.info(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(doc_files)}")
        
        successful = 0
        skipped = 0
        failed = 0
        errors = []
        
        for file_path in doc_files:
            result = self.process_single_file(file_path, force=force)
            
            if result["status"] == "success":
                successful += 1
            elif result["status"] == "skipped":
                skipped += 1
            else:
                failed += 1
                errors.append(f"{file_path.name}: {result.get('error', 'Unknown error')}")
        
        # ã‚µãƒãƒªãƒ¼çµæœ
        processing_result = {
            "processed_at": datetime.now().isoformat(),
            "total_files": len(doc_files),
            "successful": successful,
            "skipped": skipped,
            "failed": failed,
            "processing_time_seconds": (datetime.now() - start_time).total_seconds(),
            "errors": errors
        }
        
        # ã‚µãƒãƒªãƒ¼ãƒ­ã‚°å‡ºåŠ›
        end_time = datetime.now()
        duration = end_time - start_time
        logger.info(f"""
=== äº‹å‰å‡¦ç†å®Œäº† ===
å‡¦ç†æ™‚é–“: {duration.total_seconds():.1f}ç§’
æˆåŠŸ: {successful}ä»¶
ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶
å¤±æ•—: {failed}ä»¶
çµæœä¿å­˜å…ˆ: {self.results_dir}
        """)
        
        return processing_result
    
    def _get_all_document_files(self) -> List[Path]:
        """SharePointãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        doc_dir = Path(SHAREPOINT_DOCS_DIR)
        if not doc_dir.exists():
            logger.warning(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {doc_dir}")
            return []
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ‹¡å¼µå­
        supported_extensions = {'.txt', '.pdf', '.docx', '.xlsx'}
        files = []
        
        for ext in supported_extensions:
            files.extend(doc_dir.rglob(f"*{ext}"))
        
        return sorted(files)
    
    def _serialize_report(self, report) -> Dict[str, Any]:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "file_path": report.file_path,
            "file_name": report.file_name,
            "content_preview": report.content[:200] + "..." if len(report.content) > 200 else report.content,
            "report_type": report.report_type.value if report.report_type else None,
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆãƒãƒ«ãƒæˆ¦ç•¥ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            "project_id": getattr(report, 'project_id', None),
            "project_mapping_info": getattr(report, 'project_mapping_info', None),
            
            # ğŸ¯ æ–°ãƒ•ãƒ©ã‚°ä½“ç³»ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            "status_flag": report.status_flag.value if report.status_flag else None,
            "category_labels": [cat.value for cat in report.category_labels] if report.category_labels else None,
            "risk_level": report.risk_level.value if report.risk_level else None,
            
            # ğŸš¨ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "has_unexpected_values": getattr(report, 'has_unexpected_values', False),
            "validation_issues": getattr(report, 'validation_issues', []),
            
            # ğŸ¤– çµ±åˆåˆ†æçµæœãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "requires_human_review": getattr(report, 'requires_human_review', False),
            "analysis_confidence": getattr(report, 'analysis_confidence', 0.0),
            "analysis_notes": getattr(report, 'analysis_notes', None),
            
            # ğŸ” å»ºè¨­å·¥ç¨‹æƒ…å ±
            "current_construction_phase": getattr(report, 'current_construction_phase', None),
            "construction_progress": getattr(report, 'construction_progress', None),
            
            "analysis_result": {
                "key_points": ",".join(report.analysis_result.key_points) if report.analysis_result and report.analysis_result.key_points else "",
                "recommended_flags": ",".join(report.analysis_result.recommended_flags) if report.analysis_result and report.analysis_result.recommended_flags else "",
                "confidence": report.analysis_result.confidence if report.analysis_result else 0.0
            } if report.analysis_result else None,
            "anomaly_detection": {
                "has_anomaly": report.anomaly_detection.has_anomaly if report.anomaly_detection else False,
                "anomaly_score": report.anomaly_detection.anomaly_score if report.anomaly_detection else 0.0,
                "explanation": report.anomaly_detection.explanation if report.anomaly_detection else ""
            } if report.anomaly_detection else None,
            "processed_at": datetime.now().isoformat()
        }
    


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆäº‹å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--provider", 
        choices=["ollama", "openai", "anthropic"],
        default="ollama",
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ollama)"
    )
    parser.add_argument(
        "--force", "-f",
        action="store_true",
        help="å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å†å‡¦ç†ã™ã‚‹"
    )
    parser.add_argument(
        "--file",
        type=str,
        help="ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åæŒ‡å®šï¼‰"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # --forceã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ã®ç¢ºèª
    if args.force and not args.file:
        print("âš ï¸  --forceã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚")
        print("   ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå‰Šé™¤ã•ã‚Œã¾ã™:")
        print("   - data/processed_reports/ (å…¨ã¦ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«)")
        print("   - vector_store/ (å…¨ã¦ã®ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿)")
        print()
        
        # å‰Šé™¤å¯¾è±¡ã®ç¢ºèª
        processed_dir = Path("data/processed_reports")
        vector_dir = Path("vector_store")
        
        if processed_dir.exists():
            file_count = len(list(processed_dir.glob("*.json")))
            print(f"   ğŸ“„ å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆ: {file_count}ä»¶")
        
        if vector_dir.exists():
            vector_size = sum(f.stat().st_size for f in vector_dir.rglob('*') if f.is_file()) / (1024*1024)
            print(f"   ğŸ—‚ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢: {vector_size:.1f}MB")
        
        print()
        confirmation = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes']:
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚")
            return 0
        
        # ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Ÿè¡Œ
        import shutil
        if processed_dir.exists():
            shutil.rmtree(processed_dir)
            print("ğŸ—‘ï¸  data/processed_reports/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        if vector_dir.exists():
            shutil.rmtree(vector_dir)
            print("ğŸ—‘ï¸  vector_store/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        print()
    
    # äº‹å‰å‡¦ç†å®Ÿè¡Œ
    preprocessor = PreprocessingService(llm_provider=args.provider)
    
    if args.file:
        # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
        file_path = Path(SHAREPOINT_DOCS_DIR).rglob(args.file)
        file_path = next(file_path, None)
        if file_path:
            result = preprocessor.process_single_file(file_path, force=args.force)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ: {result}")
        else:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.file}")
            return 1
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        result = preprocessor.process_all_documents(force=args.force)
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== äº‹å‰å‡¦ç†çµæœ ===")
    
    if args.file:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {args.file}")
        print(f"å‡¦ç†çµæœ: {result.get('status', 'unknown')}")
        if 'processing_time' in result:
            print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.1f}ç§’")
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        print(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result['total_files']}")
        print(f"æˆåŠŸ: {result['successful']}")
        print(f"ã‚¹ã‚­ãƒƒãƒ—: {result.get('skipped', 0)}")
        print(f"å¤±æ•—: {result['failed']}")
        print(f"å‡¦ç†æ™‚é–“: {result['processing_time_seconds']:.1f}ç§’")
    
    if result.get('errors'):
        print(f"\nã‚¨ãƒ©ãƒ¼:")
        for error in result['errors']:
            print(f"  - {error}")
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰ï¼ˆå¤±æ•—ãŒã‚ã£ãŸå ´åˆã¯1ã€æˆåŠŸã®å ´åˆã¯0ï¼‰
    if args.file:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        return 0 if result.get('status') != 'error' else 1
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        return 0 if result.get('failed', 0) == 0 else 1

if __name__ == "__main__":
    sys.exit(main())