#!/usr/bin/env python3
"""
äº‹å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: ãƒ¬ãƒãƒ¼ãƒˆæŠ•å…¥æ™‚ã®å‡¦ç†ã‚’å®Ÿè¡Œ
ä½¿ç”¨æ–¹æ³•: 
  python scripts/preprocess_documents.py [--provider ollama]
  python scripts/preprocess_documents.py --force  # å…¨ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã—ã¦å†å‡¦ç†
  python scripts/preprocess_documents.py --clear-summaries  # å ±å‘Šæ›¸è¦ç´„ã®ã¿å‰Šé™¤
  python scripts/preprocess_documents.py --clear-integration  # çµ±åˆåˆ†æçµæœã®ã¿å‰Šé™¤
  python scripts/preprocess_documents.py --integration-only  # çµ±åˆåˆ†æã®ã¿å®Ÿè¡Œ
"""
import sys
import os
import json
import pickle
import argparse
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.document_processor import DocumentProcessor
from app.services.vector_store import VectorStoreService
from app.services.project_context_analyzer import ProjectContextAnalyzer
from app.config.settings import SHAREPOINT_DOCS_DIR

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingService:
    """äº‹å‰å‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, llm_provider: str = "ollama"):
        self.document_processor = DocumentProcessor(llm_provider=llm_provider, create_vector_store=True)
        self.vector_store = VectorStoreService(create_mode=True)
        self.context_analyzer = ProjectContextAnalyzer()  # ğŸ†• çµ±åˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹
        self.results_dir = project_root / "data" / "processed_reports"
        self.context_results_dir = project_root / "data" / "context_analysis"  # ğŸ†• çµ±åˆåˆ†æçµæœä¿å­˜
        self.index_file = self.results_dir / "index.json"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.context_results_dir.mkdir(parents=True, exist_ok=True)
    
    def _load_index(self) -> Dict[str, Any]:
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        if self.index_file.exists():
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"processed_files": {}, "last_updated": datetime.now().isoformat(), "version": "1.0"}
    
    def _save_index(self, index: Dict[str, Any]):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜"""
        index["last_updated"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰"""
        import hashlib
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _is_file_processed(self, file_path: Path, index: Dict[str, Any]) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        file_key = str(file_path.relative_to(project_root))
        if file_key not in index["processed_files"]:
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’ãƒãƒƒã‚·ãƒ¥ã§æ¤œå‡º
        current_hash = self._get_file_hash(file_path)
        stored_info = index["processed_files"][file_key]
        return stored_info.get("file_hash") == current_hash
    
    def process_single_file(self, file_path: Path, force: bool = False) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰å‡¦ç†"""
        index = self._load_index()
        file_key = str(file_path.relative_to(project_root))
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
        if not force and self._is_file_processed(file_path, index):
            logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰: {file_path.name}")
            return {"status": "skipped", "reason": "already_processed"}
        
        try:
            logger.info(f"ğŸ”„ å‡¦ç†ä¸­: {file_path.name}")
            start_time = datetime.now()
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
            report = self.document_processor.process_single_document(file_path)
            
            if report:
                # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                self.vector_store.add_document(
                    content=report.content,
                    metadata={
                        "file_path": str(file_path),
                        "file_name": file_path.name,
                        "report_type": report.report_type.value if report.report_type else "unknown",
                        "processed_at": datetime.now().isoformat(),
                        "flags": ",".join([flag.value for flag in report.flags]) if report.flags else "",
                        "risk_level": report.risk_level.value if report.risk_level else "ä½",
                        "has_anomaly": report.anomaly_detection.is_anomaly if report.anomaly_detection else False
                    }
                )
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦çµæœä¿å­˜
                result_data = self._serialize_report(report)
                result_file = self.results_dir / f"{file_path.stem}.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                # ğŸš€ ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆé«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ï¼‰
                cache_file = self.results_dir / f"{file_path.stem}.cache"
                try:
                    with open(cache_file, 'wb') as f:
                        pickle.dump(report, f)
                    logger.debug(f"ğŸ’¾ Binary cache saved: {cache_file.name}")
                except Exception as e:
                    logger.warning(f"Failed to save binary cache {cache_file}: {e}")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
                processing_time = (datetime.now() - start_time).total_seconds()
                index["processed_files"][file_key] = {
                    "file_hash": self._get_file_hash(file_path),
                    "processed_at": datetime.now().isoformat(),
                    "processing_time": processing_time,
                    "result_file": str(result_file.relative_to(project_root)),
                    "cache_file": str(cache_file.relative_to(project_root)),
                    "status": "success"
                }
                self._save_index(index)
                
                logger.info(f"âœ… å‡¦ç†å®Œäº†: {file_path.name} ({processing_time:.1f}ç§’)")
                return {"status": "success", "processing_time": processing_time}
            else:
                raise Exception("Document processing failed")
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨˜éŒ²
            index["processed_files"][file_key] = {
                "file_hash": self._get_file_hash(file_path),
                "processed_at": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }
            self._save_index(index)
            
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {file_path.name} - {str(e)}")
            return {"status": "error", "error": str(e)}
        
    def process_all_documents(self, force: bool = False) -> Dict[str, Any]:
        """å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®äº‹å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        logger.info("äº‹å‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        start_time = datetime.now()
        
        # SharePointãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        doc_files = self._get_all_document_files()
        logger.info(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(doc_files)}")
        
        successful = 0
        skipped = 0
        failed = 0
        errors = []
        
        for file_path in doc_files:
            result = self.process_single_file(file_path, force=force)
            
            if result["status"] == "success":
                successful += 1
            elif result["status"] == "skipped":
                skipped += 1
            else:
                failed += 1
                errors.append(f"{file_path.name}: {result.get('error', 'Unknown error')}")
        
        # ã‚µãƒãƒªãƒ¼çµæœ
        processing_result = {
            "processed_at": datetime.now().isoformat(),
            "total_files": len(doc_files),
            "successful": successful,
            "skipped": skipped,
            "failed": failed,
            "processing_time_seconds": (datetime.now() - start_time).total_seconds(),
            "errors": errors
        }
        
        # ã‚µãƒãƒªãƒ¼ãƒ­ã‚°å‡ºåŠ›
        end_time = datetime.now()
        duration = end_time - start_time
        logger.info(f"""
=== äº‹å‰å‡¦ç†å®Œäº† ===
å‡¦ç†æ™‚é–“: {duration.total_seconds():.1f}ç§’
æˆåŠŸ: {successful}ä»¶
ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶
å¤±æ•—: {failed}ä»¶
çµæœä¿å­˜å…ˆ: {self.results_dir}
        """)
        
        return processing_result
    
    def _get_all_document_files(self) -> List[Path]:
        """SharePointãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        doc_dir = Path(SHAREPOINT_DOCS_DIR)
        if not doc_dir.exists():
            logger.warning(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {doc_dir}")
            return []
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ‹¡å¼µå­
        supported_extensions = {'.txt', '.pdf', '.docx', '.xlsx'}
        files = []
        
        for ext in supported_extensions:
            files.extend(doc_dir.rglob(f"*{ext}"))
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã¸ï¼‰
        return sorted(files, key=lambda f: f.stat().st_mtime)
    
    def _serialize_report(self, report) -> Dict[str, Any]:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "file_path": report.file_path,
            "file_name": report.file_name,
            "content": report.content,  # å…¨æ–‡ã‚’ä¿å­˜
            "content_preview": report.content[:200] + "..." if len(report.content) > 200 else report.content,  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            "report_type": report.report_type.value if report.report_type else None,
            
            # ğŸ†• å…ƒå ±å‘Šæ›¸ã®æ›´æ–°æ™‚é–“ã‚’æ˜ç¤ºçš„ã«ä¿å­˜
            "created_at": report.created_at.isoformat() if report.created_at else None,
            "original_file_mtime": report.created_at.isoformat() if report.created_at else None,  # æ˜ç¤ºçš„ãªåå‰
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆãƒãƒ«ãƒæˆ¦ç•¥ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            "project_id": getattr(report, 'project_id', None),
            "project_mapping_info": getattr(report, 'project_mapping_info', None),
            
            # ğŸ¯ æ–°ãƒ•ãƒ©ã‚°ä½“ç³»ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            "status_flag": report.status_flag.value if report.status_flag else None,
            # "category_labels" å‰Šé™¤: 15ã‚«ãƒ†ã‚´ãƒªé…å»¶ç†ç”±ä½“ç³»ã«çµ±ä¸€
            "risk_level": report.risk_level.value if report.risk_level else None,
            
            # ğŸš¨ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "has_unexpected_values": getattr(report, 'has_unexpected_values', False),
            "validation_issues": getattr(report, 'validation_issues', []),
            
            # ğŸ¤– çµ±åˆåˆ†æçµæœãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "requires_human_review": getattr(report, 'requires_human_review', False),
            "analysis_confidence": getattr(report, 'analysis_confidence', 0.0),
            # "analysis_notes" å‰Šé™¤: summaryã«çµ±åˆ
            
            # ğŸ” å»ºè¨­å·¥ç¨‹æƒ…å ±
            "current_construction_phase": getattr(report, 'current_construction_phase', None),
            "construction_progress": getattr(report, 'construction_progress', None),
            
            # ğŸš§ é…å»¶ç†ç”±æƒ…å ±ï¼ˆ15ã‚«ãƒ†ã‚´ãƒªä½“ç³»ï¼‰
            "delay_reasons": getattr(report, 'delay_reasons', []),
            
            # ğŸ¯ ç·Šæ€¥åº¦ã‚¹ã‚³ã‚¢
            "urgency_score": getattr(report, 'urgency_score', 1),
            
            # current_statuså‰Šé™¤: status_flagã§çµ±ä¸€
            
            "analysis_result": {
                "summary": report.analysis_result.summary if report.analysis_result else "",
                "issues": report.analysis_result.issues if report.analysis_result else [],
                "key_points": ",".join(report.analysis_result.key_points) if report.analysis_result and report.analysis_result.key_points else "",
                "confidence": report.analysis_result.confidence if report.analysis_result else 0.0
            } if report.analysis_result else None,
            "anomaly_detection": {
                "is_anomaly": report.anomaly_detection.is_anomaly if report.anomaly_detection else False,
                "anomaly_description": report.anomaly_detection.anomaly_description if report.anomaly_detection else "",
                "confidence": report.anomaly_detection.confidence if report.anomaly_detection else 0.0,
                "suggested_action": report.anomaly_detection.suggested_action if report.anomaly_detection else "",
                "requires_human_review": report.anomaly_detection.requires_human_review if report.anomaly_detection else False
            } if report.anomaly_detection else None,
            "processed_at": datetime.now().isoformat()
        }
    
    def run_context_analysis(self, reports: List[Any]) -> Dict[str, Any]:
        """çµ±åˆåˆ†æã‚’å®Ÿè¡Œï¼ˆæœ€æ–°å ±å‘Šæ›¸ãŒè¿½åŠ ã•ã‚ŒãŸæ¡ˆä»¶ã®ã¿ï¼‰"""
        logger.info("ğŸ”„ çµ±åˆåˆ†æã‚’é–‹å§‹...")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        projects_map = {}
        for report in reports:
            project_id = getattr(report, 'project_id', None)
            if project_id and project_id != 'ä¸æ˜':
                if project_id not in projects_map:
                    projects_map[project_id] = []
                projects_map[project_id].append(report)
        
        # æ—¢å­˜ã®çµ±åˆåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿
        existing_analysis = self._load_existing_context_analysis()
        
        analysis_results = {}
        updated_projects = []
        
        for project_id, project_reports in projects_map.items():
            # æœ€æ–°å ±å‘Šæ›¸ã®æ—¥ä»˜ã‚’ç¢ºèª
            latest_report_date = max(
                (r.created_at for r in project_reports if hasattr(r, 'created_at') and r.created_at),
                default=None
            )
            
            # æ—¢å­˜åˆ†æã®æœ€çµ‚æ›´æ–°æ—¥ã¨æ¯”è¼ƒ
            existing_date = existing_analysis.get(project_id, {}).get('last_updated')
            should_update = True
            
            if existing_date and latest_report_date:
                try:
                    existing_datetime = datetime.fromisoformat(existing_date.replace('Z', '+00:00'))
                    should_update = latest_report_date > existing_datetime
                except:
                    should_update = True
            
            if should_update:
                logger.info(f"ğŸ”„ çµ±åˆåˆ†æå®Ÿè¡Œ: {project_id} ({len(project_reports)}ä»¶ã®å ±å‘Šæ›¸)")
                
                try:
                    # DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                    document_reports = []
                    for report in project_reports:
                        if hasattr(report, 'report_type'):  # æ—¢ã«DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                            document_reports.append(report)
                        else:  # è¾æ›¸å½¢å¼ã®å ´åˆã¯å¤‰æ›ãŒå¿…è¦
                            # ã“ã“ã§ã¯æ—¢ã«DocumentReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ä»®å®š
                            document_reports.append(report)
                    
                    # çµ±åˆåˆ†æå®Ÿè¡Œ
                    context_analysis = self.context_analyzer.analyze_project_context(project_id, document_reports)
                    
                    if context_analysis:
                        analysis_results[project_id] = {
                            'project_id': context_analysis.project_id,
                            'overall_status': context_analysis.overall_status.value,
                            'overall_risk': context_analysis.overall_risk.value,
                            'current_phase': context_analysis.current_phase,
                            'construction_phases': context_analysis.construction_phases,
                            'progress_trend': context_analysis.progress_trend,
                            'issue_continuity': context_analysis.issue_continuity,
                            'report_frequency': context_analysis.report_frequency,
                            'analysis_confidence': context_analysis.analysis_confidence,
                            'analysis_summary': context_analysis.analysis_summary,
                            'recommended_actions': context_analysis.recommended_actions,
                            'delay_reasons_management': context_analysis.delay_reasons_management,
                            'confidence_details': context_analysis.confidence_details,
                            'evidence_details': context_analysis.evidence_details,
                            'last_updated': datetime.now().isoformat(),
                            'reports_count': len(document_reports)
                        }
                        updated_projects.append(project_id)
                        logger.info(f"âœ… çµ±åˆåˆ†æå®Œäº†: {project_id}")
                    else:
                        logger.warning(f"âš ï¸ çµ±åˆåˆ†æå¤±æ•—: {project_id}")
                        
                except Exception as e:
                    logger.error(f"âŒ çµ±åˆåˆ†æã‚¨ãƒ©ãƒ¼: {project_id} - {e}")
            else:
                logger.info(f"â­ï¸ çµ±åˆåˆ†æã‚¹ã‚­ãƒƒãƒ—: {project_id} (æœ€æ–°)")
                # æ—¢å­˜ã®åˆ†æçµæœã‚’ä¿æŒ
                analysis_results[project_id] = existing_analysis[project_id]
        
        # çµ±åˆåˆ†æçµæœã‚’ä¿å­˜
        self._save_context_analysis(analysis_results)
        
        # ğŸ†• çµ±åˆåˆ†æçµæœã‚’ãƒ™ã‚¯ã‚¿ãƒ¼DBã«ä¿å­˜
        self._save_context_analysis_to_vector_store(analysis_results, updated_projects)
        
        logger.info(f"ğŸ‰ çµ±åˆåˆ†æå®Œäº†: {len(updated_projects)}ä»¶æ›´æ–°, {len(analysis_results)}ä»¶ç·æ•°")
        
        return {
            'total_projects': len(analysis_results),
            'updated_projects': len(updated_projects),
            'updated_project_ids': updated_projects
        }
    
    def _load_existing_context_analysis(self) -> Dict[str, Any]:
        """æ—¢å­˜ã®çµ±åˆåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿"""
        context_file = self.context_results_dir / "context_analysis.json"
        if context_file.exists():
            try:
                with open(context_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"çµ±åˆåˆ†æçµæœã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return {}
    
    def _save_context_analysis(self, analysis_results: Dict[str, Any]):
        """çµ±åˆåˆ†æçµæœã‚’ä¿å­˜"""
        context_file = self.context_results_dir / "context_analysis.json"
        try:
            with open(context_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2)
            logger.info(f"çµ±åˆåˆ†æçµæœä¿å­˜: {context_file}")
        except Exception as e:
            logger.error(f"çµ±åˆåˆ†æçµæœä¿å­˜å¤±æ•—: {e}")
    
    def _save_context_analysis_to_vector_store(self, analysis_results: Dict[str, Any], updated_projects: List[str]):
        """çµ±åˆåˆ†æçµæœã‚’ãƒ™ã‚¯ã‚¿ãƒ¼DBã«ä¿å­˜"""
        try:
            logger.info(f"ğŸ”„ çµ±åˆåˆ†æçµæœã‚’ãƒ™ã‚¯ã‚¿ãƒ¼DBã«ä¿å­˜ä¸­...")
            
            success_count = 0
            for project_id in updated_projects:
                if project_id in analysis_results:
                    analysis_data = analysis_results[project_id]
                    if self.vector_store.add_context_analysis(project_id, analysis_data):
                        success_count += 1
                    else:
                        logger.warning(f"âš ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼DBä¿å­˜å¤±æ•—: {project_id}")
            
            logger.info(f"âœ… çµ±åˆåˆ†æçµæœã®ãƒ™ã‚¯ã‚¿ãƒ¼DBä¿å­˜å®Œäº†: {success_count}/{len(updated_projects)}ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ çµ±åˆåˆ†æçµæœã®ãƒ™ã‚¯ã‚¿ãƒ¼DBä¿å­˜ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_all_processed_reports(self) -> List[Any]:
        """å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆã‚’å…¨ã¦èª­ã¿è¾¼ã¿"""
        reports = []
        
        # ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        binary_cache_file = self.results_dir / "processed_reports.pkl"
        if binary_cache_file.exists():
            try:
                with open(binary_cache_file, 'rb') as f:
                    reports = pickle.load(f)
                logger.info(f"ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(reports)}ä»¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿")
                return reports
            except Exception as e:
                logger.warning(f"ãƒã‚¤ãƒŠãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        for json_file in self.results_dir.glob("*.json"):
            if json_file.name == "index.json":
                continue
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                    # ç°¡æ˜“çš„ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆï¼ˆçµ±åˆåˆ†æç”¨ï¼‰
                    from types import SimpleNamespace
                    report = SimpleNamespace(**report_data)
                    
                    # ğŸ†• å…ƒå ±å‘Šæ›¸ã®æ›´æ–°æ™‚é–“ã‚’æ­£ç¢ºã«å¾©å…ƒ
                    # 1. ã¾ãšæ˜ç¤ºçš„ã«ä¿å­˜ã•ã‚ŒãŸ original_file_mtime ã‚’ç¢ºèª
                    if hasattr(report, 'original_file_mtime') and report.original_file_mtime:
                        try:
                            report.created_at = datetime.fromisoformat(report.original_file_mtime.replace('Z', '+00:00'))
                        except:
                            report.created_at = datetime.min
                    # 2. æ¬¡ã« created_at ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç¢ºèª
                    elif hasattr(report, 'created_at') and isinstance(report.created_at, str):
                        try:
                            report.created_at = datetime.fromisoformat(report.created_at.replace('Z', '+00:00'))
                        except:
                            report.created_at = datetime.min
                    # 3. ã©ã¡ã‚‰ã‚‚å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    elif not hasattr(report, 'created_at') or report.created_at is None:
                        try:
                            file_path = Path(report.file_path) if hasattr(report, 'file_path') else json_file
                            report.created_at = datetime.fromtimestamp(file_path.stat().st_mtime)
                            logger.info(f"Using file mtime as fallback for {report.file_name if hasattr(report, 'file_name') else 'unknown'}")
                        except:
                            report.created_at = datetime.min
                    
                    # ãã®ä»–ã®å¿…è¦ãªå±æ€§ã‚’ç¢ºä¿
                    if not hasattr(report, 'project_id'):
                        report.project_id = getattr(report, 'project_id', 'ä¸æ˜')
                    if not hasattr(report, 'report_type'):
                        report.report_type = getattr(report, 'report_type', 'OTHER')
                    
                    reports.append(report)
            except Exception as e:
                logger.warning(f"ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {json_file} - {e}")
        
        logger.info(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{len(reports)}ä»¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿")
        return reports


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆäº‹å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--provider", 
        choices=["ollama", "openai", "anthropic"],
        default="ollama",
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ollama)"
    )
    parser.add_argument(
        "--force", "-f",
        action="store_true",
        help="å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å†å‡¦ç†ã™ã‚‹"
    )
    parser.add_argument(
        "--file",
        type=str,
        help="ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åæŒ‡å®šï¼‰"
    )
    parser.add_argument(
        "--integration-only",
        action="store_true",
        help="çµ±åˆåˆ†æã®ã¿å®Ÿè¡Œã™ã‚‹ï¼ˆå ±å‘Šæ›¸å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰"
    )
    parser.add_argument(
        "--clear-summaries",
        action="store_true",
        help="å ±å‘Šæ›¸è¦ç´„çµæœã®ã¿ã‚’å‰Šé™¤ã™ã‚‹"
    )
    parser.add_argument(
        "--clear-integration",
        action="store_true",
        help="çµ±åˆåˆ†æçµæœã®ã¿ã‚’å‰Šé™¤ã™ã‚‹"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å€‹åˆ¥å‰Šé™¤ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å‡¦ç†
    if args.clear_summaries:
        print("âš ï¸  --clear-summariesã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚")
        print("   å ±å‘Šæ›¸è¦ç´„çµæœã‚’å‰Šé™¤ã—ã¾ã™:")
        print("   - data/processed_reports/ (å…¨ã¦ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«)")
        print("   - vector_store/ (å…¨ã¦ã®ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿)")
        print()
        
        processed_dir = Path("data/processed_reports")
        vector_dir = Path("vector_store")
        
        if processed_dir.exists():
            file_count = len(list(processed_dir.glob("*.json")))
            print(f"   ğŸ“„ å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆ: {file_count}ä»¶")
        
        if vector_dir.exists():
            vector_size = sum(f.stat().st_size for f in vector_dir.rglob('*') if f.is_file()) / (1024*1024)
            print(f"   ğŸ—‚ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢: {vector_size:.1f}MB")
        
        print()
        confirmation = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes']:
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚")
            return 0
        
        import shutil
        if processed_dir.exists():
            shutil.rmtree(processed_dir)
            print("ğŸ—‘ï¸  data/processed_reports/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        if vector_dir.exists():
            shutil.rmtree(vector_dir)
            print("ğŸ—‘ï¸  vector_store/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        print("âœ… å ±å‘Šæ›¸è¦ç´„çµæœã®å‰Šé™¤ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return 0
    
    if args.clear_integration:
        print("âš ï¸  --clear-integrationã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚")
        print("   çµ±åˆåˆ†æçµæœã‚’å‰Šé™¤ã—ã¾ã™:")
        print("   - data/context_analysis/ (å…¨ã¦ã®çµ±åˆåˆ†æçµæœ)")
        print()
        
        context_dir = Path("data/context_analysis")
        
        if context_dir.exists():
            file_count = len(list(context_dir.glob("*.json")))
            print(f"   ğŸ“Š çµ±åˆåˆ†æçµæœ: {file_count}ä»¶")
        
        print()
        confirmation = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes']:
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚")
            return 0
        
        import shutil
        if context_dir.exists():
            shutil.rmtree(context_dir)
            print("ğŸ—‘ï¸  data/context_analysis/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        print("âœ… çµ±åˆåˆ†æçµæœã®å‰Šé™¤ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return 0

    # --forceã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ã®ç¢ºèª
    if args.force and not args.file:
        print("âš ï¸  --forceã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚")
        print("   ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå‰Šé™¤ã•ã‚Œã¾ã™:")
        print("   - data/processed_reports/ (å…¨ã¦ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«)")
        print("   - data/context_analysis/ (å…¨ã¦ã®çµ±åˆåˆ†æçµæœ)")
        print("   - vector_store/ (å…¨ã¦ã®ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿)")
        print("   - data/confirmed_mappings.json (ç¢ºå®šæ¸ˆã¿ãƒãƒƒãƒ”ãƒ³ã‚°)")
        print()
        
        # å‰Šé™¤å¯¾è±¡ã®ç¢ºèª
        processed_dir = Path("data/processed_reports")
        context_dir = Path("data/context_analysis")
        vector_dir = Path("vector_store")
        confirmed_mappings_file = Path("data/confirmed_mappings.json")
        
        if processed_dir.exists():
            file_count = len(list(processed_dir.glob("*.json")))
            print(f"   ğŸ“„ å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆ: {file_count}ä»¶")
        
        if context_dir.exists():
            context_count = len(list(context_dir.glob("*.json")))
            print(f"   ğŸ“Š çµ±åˆåˆ†æçµæœ: {context_count}ä»¶")
        
        if vector_dir.exists():
            vector_size = sum(f.stat().st_size for f in vector_dir.rglob('*') if f.is_file()) / (1024*1024)
            print(f"   ğŸ—‚ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢: {vector_size:.1f}MB")
        
        if confirmed_mappings_file.exists():
            print(f"   ğŸ“‹ ç¢ºå®šæ¸ˆã¿ãƒãƒƒãƒ”ãƒ³ã‚°: å­˜åœ¨")
        
        print()
        confirmation = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes']:
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚")
            return 0
        
        # ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Ÿè¡Œ
        import shutil
        if processed_dir.exists():
            shutil.rmtree(processed_dir)
            print("ğŸ—‘ï¸  data/processed_reports/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        if context_dir.exists():
            shutil.rmtree(context_dir)
            print("ğŸ—‘ï¸  data/context_analysis/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        if vector_dir.exists():
            shutil.rmtree(vector_dir)
            print("ğŸ—‘ï¸  vector_store/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        if confirmed_mappings_file.exists():
            confirmed_mappings_file.unlink()
            print("ğŸ—‘ï¸  data/confirmed_mappings.json ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        print()
    
    # äº‹å‰å‡¦ç†å®Ÿè¡Œ
    preprocessor = PreprocessingService(llm_provider=args.provider)
    
    if args.integration_only:
        # çµ±åˆåˆ†æã®ã¿å®Ÿè¡Œ
        print("=== çµ±åˆåˆ†æã®ã¿å®Ÿè¡Œ ===")
        start_time = time.time()
        try:
            reports = preprocessor.load_all_processed_reports()
            context_result = preprocessor.run_context_analysis(reports)
            end_time = time.time()
            
            print("çµ±åˆåˆ†æçµæœ:")
            print(f"  ç·æ¡ˆä»¶æ•°: {context_result['total_projects']}")
            print(f"  æ›´æ–°æ¡ˆä»¶æ•°: {context_result['updated_projects']}")
            print(f"  å‡¦ç†æ™‚é–“: {end_time - start_time:.1f}ç§’")
            if context_result['updated_project_ids']:
                print(f"  æ›´æ–°æ¡ˆä»¶ID: {', '.join(context_result['updated_project_ids'])}")
            
            return 0
        except Exception as e:
            print(f"âŒ çµ±åˆåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return 1
    elif args.file:
        # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
        file_path = Path(SHAREPOINT_DOCS_DIR).rglob(args.file)
        file_path = next(file_path, None)
        if file_path:
            result = preprocessor.process_single_file(file_path, force=args.force)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ: {result}")
        else:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.file}")
            return 1
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        result = preprocessor.process_all_documents(force=args.force)
        
        # ğŸ†• çµ±åˆåˆ†æå®Ÿè¡Œï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆã®ã¿ï¼‰
        if result.get('successful', 0) > 0:
            print(f"\n=== çµ±åˆåˆ†æå®Ÿè¡Œ ===")
            try:
                # å‡¦ç†æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
                reports = preprocessor.load_all_processed_reports()
                context_result = preprocessor.run_context_analysis(reports)
                
                print(f"çµ±åˆåˆ†æçµæœ:")
                print(f"  ç·æ¡ˆä»¶æ•°: {context_result['total_projects']}")
                print(f"  æ›´æ–°æ¡ˆä»¶æ•°: {context_result['updated_projects']}")
                if context_result['updated_project_ids']:
                    print(f"  æ›´æ–°æ¡ˆä»¶ID: {', '.join(context_result['updated_project_ids'])}")
                
                result['context_analysis'] = context_result
                
            except Exception as e:
                print(f"âš ï¸ çµ±åˆåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                result['context_analysis_error'] = str(e)
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== äº‹å‰å‡¦ç†çµæœ ===")
    
    if args.file:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {args.file}")
        print(f"å‡¦ç†çµæœ: {result.get('status', 'unknown')}")
        if 'processing_time' in result:
            print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.1f}ç§’")
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        print(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result['total_files']}")
        print(f"æˆåŠŸ: {result['successful']}")
        print(f"ã‚¹ã‚­ãƒƒãƒ—: {result.get('skipped', 0)}")
        print(f"å¤±æ•—: {result['failed']}")
        print(f"å‡¦ç†æ™‚é–“: {result['processing_time_seconds']:.1f}ç§’")
    
    if result.get('errors'):
        print(f"\nã‚¨ãƒ©ãƒ¼:")
        for error in result['errors']:
            print(f"  - {error}")
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰ï¼ˆå¤±æ•—ãŒã‚ã£ãŸå ´åˆã¯1ã€æˆåŠŸã®å ´åˆã¯0ï¼‰
    if args.file:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        return 0 if result.get('status') != 'error' else 1
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
        return 0 if result.get('failed', 0) == 0 else 1

if __name__ == "__main__":
    sys.exit(main())